{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34789f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2315322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d99502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.where(x >= 0, \n",
    "                        x, \n",
    "                        0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94c5e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(activation):#maybe getderivatives here iteself ?\n",
    "    def sigmoid(x):\n",
    "        return np.where(x >= 0, \n",
    "                        1 / (1 + np.exp(-x)), \n",
    "                        np.exp(x) / (1 + np.exp(x)))\n",
    "    def softmax(x):\n",
    "        z=x-np.max(x,axis=0)\n",
    "        return np.exp(z)/np.sum(np.exp(z),axis=0)\n",
    "    def relu(x):\n",
    "        rel=np.where(x >= 0, \n",
    "                            x, \n",
    "                            0)\n",
    "        return rel\n",
    "    if activation=='sigmoid':\n",
    "        return sigmoid\n",
    "    elif activation=='softmax':\n",
    "        return softmax\n",
    "    elif activation== 'tanh':\n",
    "        return np.tanh\n",
    "    elif activation== 'relu':\n",
    "        return relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f384261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_derivative(activation):#maybe getderivatives here iteself ?\n",
    "    def sigmoid_d(x):\n",
    "        sig= np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "        return sig*(1-sig)\n",
    "    def softmax_d(x):\n",
    "        z=x-np.max(x,axis=0)\n",
    "        soft=np.exp(z)/np.sum(np.exp(z),axis=0)\n",
    "        return soft*(1-soft)\n",
    "    def tanh_d(x):\n",
    "        return 1-np.tanh(x)**2\n",
    "    def relu_d(x):\n",
    "        return np.where(x >= 0, \n",
    "                            x, \n",
    "                            0)\n",
    "    \n",
    "    if activation=='sigmoid':\n",
    "        return sigmoid_d\n",
    "    elif activation=='softmax':\n",
    "        '''\n",
    "        need to think more, not required for backprop as we look directly at dL/da_l\n",
    "        But still, for the sake of completeness, and if user wants softmax in the middle?\n",
    "        d S(x_i) /d x_j= S(x_i)*(kronecker delta_i,j -S(x_j))\n",
    "        But we care about only dh_k,j/da_k,j So no need to implement d S(x_i) /d x_j\n",
    "        d S(x_i) /d x_i should suffice\n",
    "        so we get array of [ d S(x_1) /d x_1, d S(x_2) /d x_2, ....]\n",
    "        \n",
    "        For MSE loss after softmax, we need cross terms...\n",
    "        '''\n",
    "        \n",
    "        return softmax_d\n",
    "    elif activation=='tanh':\n",
    "        return tanh_d\n",
    "    elif activation=='relu':\n",
    "        return relu_d\n",
    "    assert(activation=='relu'or activation=='tanh'or activation=='sigmoid' or activation=='softmax'), 'Must be \\'relu\\'or \\'tanh\\' or \\'sigmoid\\' or \\'softmax\\' '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c0621d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(loss='cross-entropy'):\n",
    "    \n",
    "   \n",
    "    \n",
    "        \n",
    "    def crossentropy(P,Q):\n",
    "        assert(P.shape==Q.shape), \"Inputs must be of same shape\"\n",
    "\n",
    "        return np.sum([-np.dot(P[:,i],np.log2(Q[:,i])) for i in range(P.shape[1])])\n",
    "    def SE(P,Q):\n",
    "        assert(P.shape==Q.shape), \"Inputs must be of same shape\"\n",
    "\n",
    "        return np.sum(np.square(P-Q))\n",
    "    \n",
    "    if loss==\"SE\":\n",
    "        return SE\n",
    "    return crossentropy\n",
    "    \n",
    "    \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a254663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_derivative(loss):\n",
    "    def SE_d(y_in,y_pred_in):\n",
    "        '''\n",
    "        derivative of MSE after softmax is used to get probabs from a_L:\n",
    "        We need indicator because the all terms of y_true are required unlike cross-entropy where only y_pred[l] is required\n",
    "        Thus transforming the stacked indicator to y_true, not here...\n",
    "        \n",
    "        '''\n",
    "\n",
    "        def indicator(i,j):\n",
    "                if i==j:\n",
    "                    return 1\n",
    "                return 0\n",
    "\n",
    "\n",
    "        assert(y_in.shape[0]==y_pred_in.shape[0]),\"Inputs must contain same number of examples\"\n",
    "\n",
    "        y=y_in.ravel()\n",
    "        y_pred=y_pred_in.ravel()\n",
    "\n",
    "\n",
    "        return np.array([\n",
    "            [2*np.sum([(y_pred[i]-y[i])*y[i]*(indicator(i,j) - y_pred[j]) for i in range(y.shape[0])])]\n",
    "            for j in range(len(y))\n",
    "        ])    \n",
    "   \n",
    "    \n",
    "        \n",
    "    def crossentropy_d(y,y_pred):\n",
    "        \n",
    "\n",
    "        return -(y-y_pred)\n",
    "    \n",
    "    \n",
    "    if loss==\"cross-entropy\":\n",
    "        return crossentropy_d\n",
    "    return SE_d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c48942c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self,input_size,output_size,activation='sigmoid',type_='random'):\n",
    "            \n",
    "        ''' \n",
    "        output size number of neurons i\n",
    "        input size j\n",
    "        \n",
    "        '''\n",
    "        assert(type_=='random'or type_=='xavier'or type=='glorot' or type=='He' ), 'Must be \\'random\\'or \\'xavier\\' or \\'glorot\\' or \\'He\\' '\n",
    "        \n",
    "        if type_=='random':\n",
    "            scale=0.01\n",
    "            self.W=np.random.randn(output_size,input_size)*scale #size ixj\n",
    "            self.b=np.zeros((output_size,1))         #size i\n",
    "            \n",
    "        elif type_=='xavier' or type_=='glorot':\n",
    "            # Xavier Uniform\n",
    "            r=np.sqrt(6/(input_size+output_size))\n",
    "            self.W=np.random.uniform(-r,r,(output_size,input_size))\n",
    "            self.b=np.zeros((output_size,1))\n",
    "            \n",
    "        else:#He\n",
    "            self.W= np.random.randn(output_size,input_size)*np.sqrt(2/input_size)\n",
    "            self.b=np.zeros((output_size,1))\n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        self.a=np.zeros((output_size,1))          #size i\n",
    "        self.h=np.zeros((output_size,1))         #size i\n",
    "        self.g=get_activation(activation)\n",
    "        \n",
    "        self.d_a=np.zeros((output_size,1))\n",
    "        self.d_h=np.zeros((output_size,1))\n",
    "        self.d_W=np.zeros((output_size,input_size))\n",
    "        self.d_b=np.zeros((output_size,1))\n",
    "        self.d_g=get_activation_derivative(activation)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.a=self.b+np.matmul(self.W,inputs)\n",
    "        self.h=self.g(self.a)\n",
    "        return self.h\n",
    "    def reset(self):\n",
    "        self.d_a=np.zeros(np.shape(self.d_a))\n",
    "        self.d_h=np.zeros(np.shape(self.d_h))\n",
    "        self.d_W=np.zeros(np.shape(self.d_W))\n",
    "        self.d_b=np.zeros(np.shape(self.d_b))\n",
    "        \n",
    "    def hard_set(self,W,b):#hardsets the weight. useful for debugging\n",
    "        self.W=W\n",
    "        self.b=b\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "06f18d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,X_size,Y_size,hidden_layer_sizes=[4],hidden_layer_activations=['sigmoid'],loss='cross-entropy',lamdba_m=0):\n",
    "        '''\n",
    "        '''   \n",
    "        \n",
    "        self.input_size=X_size\n",
    "        self.output_size=Y_size\n",
    "        self.hidden_layer_sizes=hidden_layer_sizes\n",
    "        self.layers=[]\n",
    "        \n",
    "        prev_size=self.input_size\n",
    "    \n",
    "        for size,activation in zip(hidden_layer_sizes,hidden_layer_activations):\n",
    "            self.layers.append(layer(prev_size,size,activation))\n",
    "            prev_size=size\n",
    "        self.layers.append(layer(size,self.output_size,'softmax'))\n",
    "        \n",
    "        self.loss=get_loss(loss)#without regularization term\n",
    "        self.loss_d=get_loss_derivative(loss)\n",
    "        self.lamdba_m=lamdba_m #we shall pass lambda/m to this, where m is patch size\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output=x\n",
    "        # print(output.shape)\n",
    "        for layer in  self.layers:\n",
    "            # print('W',layer.W.shape)\n",
    "            output=layer.forward(output)\n",
    "            # print(output.shape)   \n",
    "        return output\n",
    "    \n",
    "    def reset(self):\n",
    "        for layer in  self.layers:\n",
    "            # resets the dWs\n",
    "            layer.reset()\n",
    "    \n",
    " \n",
    "        \n",
    "    \n",
    "          \n",
    "            \n",
    "    def backward(self,x,y,y_pred):\n",
    "        # self.layers[-1].d_h is not needed as d_h is used to calculate d_a and self.layers[-1].h is softmax\n",
    "        self.layers[-1].d_a=self.loss_d(y,y_pred)\n",
    "            \n",
    "        \n",
    "        \n",
    "        for idx in range(len(self.layers)-1,0,-1): #goes from L->2, for l=1 we do outside\n",
    "            \n",
    "            \n",
    "            #compute gradient wrt parameters\n",
    "            self.layers[idx].d_W+=np.matmul(self.layers[idx].d_a,np.transpose(self.layers[idx-1].h))+self.lamdba_m*self.layers[idx].W\n",
    "            self.layers[idx].d_b+=self.layers[idx].d_a\n",
    "            \n",
    "            #compute gradient wrt layer below -- will help in next layer iter\n",
    "            self.layers[idx-1].d_h=np.matmul(np.transpose(self.layers[idx].W),self.layers[idx].d_a)\n",
    "            \n",
    "            #compute gradient -- element wise multiplivation, derivative of the activation function of layer idx-1\n",
    "            self.layers[idx-1].d_a=self.layers[idx-1].d_h*self.layers[idx-1].d_g(self.layers[idx-1].a)\n",
    "        assert(idx-1==0)\n",
    "                        \n",
    "        self.layers[0].d_W+=np.matmul(self.layers[0].d_a,np.transpose(x))+self.lamdba_m*self.layers[0].W\n",
    "        self.layers[0].d_b+=self.layers[0].d_a\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self,Xtest):\n",
    "        preds=[]\n",
    "        for i in range(Xtest.shape[1]):\n",
    "            preds.append(self.forward(Xtest[:,[i]]))\n",
    "        \n",
    "        ytest_pred=np.hstack(preds)\n",
    "        return ytest_pred\n",
    "    \n",
    "        \n",
    "        \n",
    "                    \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da5612df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizers:\n",
    "    def __init__(self,X_size,Y_size,hidden_layer_sizes=[4],hidden_layer_activations=['sigmoid'],\n",
    "                 loss='cross-entropy',optimizer='adam',lamdba=0,batch_size=5,epochs=10,eta=1e-3):\n",
    "        self.batch_size=batch_size\n",
    "        self.epochs=epochs\n",
    "        self.train_loss=[]\n",
    "        self.val_loss=[]\n",
    "        self.model=Model(X_size,Y_size,hidden_layer_sizes,hidden_layer_activations,loss,lamdba/self.batch_size)\n",
    "        self.learning_rate=eta/batch_size\n",
    "        self.optimizer=optimizer\n",
    "\n",
    "        \n",
    "\n",
    "    def batch_gradient_descent(self,traindat,testdat,verbose=False):\n",
    "        \n",
    "        '''\n",
    "        Mini-Batch Gradient Descent\n",
    "        at batchsize=1, behaves like sgd, batchsize=np.shape(X), behaves as gd\n",
    "        eta is the learning rate\n",
    "        '''\n",
    "        X,Y=traindat\n",
    "        \n",
    "        Xval,Yval=testdat\n",
    "        \n",
    "        \n",
    "\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "                #update if the number of points seen==batch size, or if data ends\n",
    "                if number_points_seen%self.batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                    for layer in self.model.layers:\n",
    "                        layer.W=layer.W-self.learning_rate*layer.d_W\n",
    "                        layer.b=layer.b-self.learning_rate*layer.d_b\n",
    "                    self.model.reset() #reset grads before new epoch\n",
    "            \n",
    "            #end of epoch\n",
    "            regularization=1/2*self.model.lamdba_m*np.sum([np.sum(layer.W**2) for layer in self.model.layers])\n",
    "            self.train_loss.append((self.model.loss(Y,self.model.predict(X))+regularization)/X.shape[1])\n",
    "            self.val_loss.append(self.model.loss(Yval,self.model.predict(Xval))/Xval.shape[1])\n",
    "            \n",
    "    def stochastic_gradient_descent(self,traindat,testdat,eta=1e-3,max_iters=10):\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        stochastic Gradient Descent\n",
    "        '''\n",
    "                \n",
    "        X,Y=traindat\n",
    "        \n",
    "        Xval,Yval=testdat\n",
    "\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                \n",
    "                #update\n",
    "                for layer in self.model.layers:\n",
    "                    layer.W=layer.W-self.learning_rate*layer.d_W\n",
    "                    layer.b=layer.b-self.learning_rate*layer.d_b\n",
    "                self.model.reset() #reset grads before new update\n",
    "                    \n",
    "            regularization=1/2*self.model.lamdba_m*np.sum([np.sum(layer.W**2) for layer in self.model.layers])\n",
    "            self.train_loss.append((self.model.loss(Y,self.model.predict(X))+regularization)/X.shape[1])\n",
    "            self.val_loss.append(self.model.loss(Yval,self.model.predict(Xval))/Xval.shape[1])\n",
    "        \n",
    "    def Momentum(self,traindat,testdat,beta=0.9):\n",
    "        ''''''\n",
    "        \n",
    "        X,Y=traindat\n",
    "        \n",
    "        Xval,Yval=testdat\n",
    "        u_W=[np.zeros(np.shape(layer.d_W)) for layer in self.model.layers]\n",
    "        u_b=[np.zeros(np.shape(layer.d_b)) for layer in self.model.layers]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%self.batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.model.layers)):\n",
    "                    layer=self.model.layers[i]\n",
    "                    u_W[i]=beta*u_W[i]+layer.d_W\n",
    "                    u_b[i]=beta*u_b[i]+layer.d_b\n",
    "                    layer.W=layer.W-self.learning_rate*u_W[i]\n",
    "                    layer.b=layer.b-self.learning_rate*u_b[i]\n",
    "                self.model.reset() #reset grads before new update\n",
    "                \n",
    "            regularization=1/2*self.model.lamdba_m*np.sum([np.sum(layer.W**2) for layer in self.model.layers])\n",
    "            self.train_loss.append((self.model.loss(Y,self.model.predict(X))+regularization)/X.shape[1])\n",
    "            self.val_loss.append(self.model.loss(Yval,self.model.predict(Xval))/Xval.shape[1])\n",
    "                \n",
    "\n",
    "\n",
    "    def rmsprop(self,traindat,testdat,beta=0.9):\n",
    "        ''''''\n",
    "        X,Y=traindat\n",
    "        \n",
    "        Xval,Yval=testdat\n",
    "        \n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.model.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.model.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%self.batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.model.layers)):\n",
    "                    layer=self.model.layers[i]\n",
    "                    v_W[i]=beta*v_W[i]+(1-beta)*layer.d_W**2\n",
    "                    v_b[i]=beta*v_b[i]+(1-beta)*layer.d_b**2\n",
    "                    layer.W=layer.W-(self.learning_rate/np.sqrt(v_W[i]+epsilon))*layer.d_W\n",
    "                    layer.b=layer.b-(self.learning_rate/np.sqrt(v_b[i]+epsilon))*layer.d_b\n",
    "                self.model.reset() #reset grads before new epoch\n",
    "                \n",
    "            regularization=1/2*self.model.lamdba_m*np.sum([np.sum(layer.W**2) for layer in self.model.layers])\n",
    "            self.train_loss.append((self.model.loss(Y,self.model.predict(X))+regularization)/X.shape[1])\n",
    "            self.val_loss.append(self.model.loss(Yval,self.model.predict(Xval))/Xval.shape[1])\n",
    "\n",
    "            \n",
    "    def Adam(self,traindat,testdat,beta1=0.9, beta2=0.999):\n",
    "        ''''''\n",
    "        X,Y=traindat\n",
    "        \n",
    "        Xval,Yval=testdat\n",
    "        \n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.model.layers]\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.model.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.model.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.model.layers]\n",
    "        \n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%self.batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.model.layers)):\n",
    "                    layer=self.model.layers[i]\n",
    "                    #updating momentum, velocity\n",
    "                    m_W[i]=beta1*m_W[i]+(1-beta1)*layer.d_W\n",
    "                    m_b[i]=beta1*m_b[i]+(1-beta1)*layer.d_b\n",
    "                    \n",
    "                    v_W[i]=beta2*v_W[i]+(1-beta2)*layer.d_W**2\n",
    "                    v_b[i]=beta2*v_b[i]+(1-beta2)*layer.d_b**2\n",
    "                    \n",
    "                    m_W_hat=m_W[i]/(1-np.power(beta1,t+1))\n",
    "                    m_b_hat=m_b[i]/(1-np.power(beta1,t+1))\n",
    "                    v_W_hat=v_W[i]/(1-np.power(beta2,t+1))\n",
    "                    v_b_hat=v_b[i]/(1-np.power(beta2,t+1))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    layer.W=layer.W-(self.learning_rate*m_W_hat)/(np.sqrt(v_W_hat)+epsilon)\n",
    "                    layer.b=layer.b-(self.learning_rate*m_b_hat)/(np.sqrt(v_b_hat)+epsilon)\n",
    "                self.model.reset() #reset grads before new epoch\n",
    "                \n",
    "            regularization=1/2*self.model.lamdba_m*np.sum([np.sum(layer.W**2) for layer in self.model.layers])\n",
    "            self.train_loss.append((self.model.loss(Y,self.model.predict(X))+regularization)/X.shape[1])\n",
    "            self.val_loss.append(self.model.loss(Yval,self.model.predict(Xval))/Xval.shape[1])\n",
    "    \n",
    "    def NAG(self,traindat,testdat,beta=0.9):\n",
    "        \n",
    "        ''''''\n",
    "        \n",
    "        X,Y=traindat\n",
    "        \n",
    "        \n",
    "        Xval,Yval=testdat\n",
    "        \n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.model.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.model.layers]\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%self.batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.model.layers)):\n",
    "                    layer=self.model.layers[i]\n",
    "                    m_W[i]=beta*m_W[i]+self.learning_rate*layer.d_W\n",
    "                    m_b[i]=beta*m_b[i]+self.learning_rate*layer.d_b\n",
    "\n",
    "                    \n",
    "                    layer.W=layer.W-(beta*m_W[i]+self.learning_rate*layer.d_W[i])\n",
    "                    layer.b=layer.b-(beta*m_b[i]+self.learning_rate*layer.d_b[i])\n",
    "                self.model.reset() #reset grads before new epoch\n",
    "                \n",
    "            regularization=1/2*self.model.lamdba_m*np.sum([np.sum(layer.W**2) for layer in self.model.layers])\n",
    "            self.train_loss.append((self.model.loss(Y,self.model.predict(X))+regularization)/X.shape[1])\n",
    "            self.val_loss.append(self.model.loss(Yval,self.model.predict(Xval))/Xval.shape[1])\n",
    "    \n",
    "    def NAdam(self,traindat,testdat,beta1=0.9, beta2=0.999):\n",
    "        ''''''\n",
    "        \n",
    "        X,Y=traindat\n",
    "        \n",
    "        Xval,Yval=testdat\n",
    "        \n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.model.layers]\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.model.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.model.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.model.layers]\n",
    "        \n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%self.batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.model.layers)):\n",
    "                    layer=self.model.layers[i]\n",
    "                    #updating momentum, velocity\n",
    "                    m_W[i]=beta1*m_W[i]+(1-beta1)*layer.d_W\n",
    "                    m_b[i]=beta1*m_b[i]+(1-beta1)*layer.d_b\n",
    "                    \n",
    "                    v_W[i]=beta2*v_W[i]+(1-beta2)*layer.d_W**2\n",
    "                    v_b[i]=beta2*v_b[i]+(1-beta2)*layer.d_b**2\n",
    "                    \n",
    "                    m_W_hat=m_W[i]/(1-np.power(beta1,t+1))\n",
    "                    m_b_hat=m_b[i]/(1-np.power(beta1,t+1))\n",
    "                    v_W_hat=v_W[i]/(1-np.power(beta2,t+1))\n",
    "                    v_b_hat=v_b[i]/(1-np.power(beta2,t+1))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    layer.W=layer.W-(self.learning_rate/(np.sqrt(v_W_hat)+epsilon))*\\\n",
    "                    (beta1*m_W_hat+((1-beta1)/(1-np.power(beta1,t+1)))*layer.d_W)\n",
    "                    layer.b=layer.b-(self.learning_rate/(np.sqrt(v_b_hat)+epsilon))*\\\n",
    "                    (beta1*m_b_hat+((1-beta1)/(1-np.power(beta1,t+1)))*layer.d_b)\n",
    "                    \n",
    "                self.model.reset() #reset grads before new epoch\n",
    "            regularization=1/2*self.model.lamdba_m*np.sum([np.sum(layer.W**2) for layer in self.model.layers])\n",
    "            self.train_loss.append((self.model.loss(Y,self.model.predict(X))+regularization)/X.shape[1])\n",
    "            self.val_loss.append(self.model.loss(Yval,self.model.predict(Xval))/Xval.shape[1])\n",
    "\n",
    "                \n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76db4320",
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizers_tester:\n",
    "    def __init__(self,X_size,Y_size,hidden_layer_sizes=[4],hidden_layer_activations=['sigmoid'],\n",
    "                 loss='cross-entropy',optimizer='adam',lamdba=0,batch_size=5,epochs=10,eta=1e-3):\n",
    "        self.batch_size=batch_size\n",
    "        self.epochs=epochs\n",
    "        self.train_loss=[]\n",
    "        self.val_loss=[]\n",
    "        self.model=Model(X_size,Y_size,hidden_layer_sizes,hidden_layer_activations,loss,lamdba/self.batch_size)\n",
    "        self.learning_rate=eta\n",
    "        self.optimizer=optimizer\n",
    "\n",
    "        \n",
    "\n",
    "    def batch_gradient_descent(self,traindat,testdat,verbose=False):\n",
    "        \n",
    "        '''\n",
    "        Mini-Batch Gradient Descent\n",
    "        at batchsize=1, behaves like sgd, batchsize=np.shape(X), behaves as gd\n",
    "        eta is the learning rate\n",
    "        '''\n",
    "        X,Y=traindat\n",
    "        \n",
    "        Xval,Yval=testdat\n",
    "        WTrack=[]\n",
    "        bTrack=[]\n",
    "        dWTrack=[]\n",
    "        dbTrack=[]\n",
    "        \n",
    "#         !!!!!!\n",
    "        \n",
    "\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "                #update if the number of points seen==batch size, or if data ends\n",
    "                if number_points_seen%self.batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                    dWTrack_layer=[]\n",
    "                    dbTrack_layer=[]\n",
    "                    WTrack_layer=[]\n",
    "                    bTrack_layer=[]\n",
    "                    \n",
    "                    for layer in self.model.layers:\n",
    "                        layer.W=layer.W-self.learning_rate*layer.d_W\n",
    "                        layer.b=layer.b-self.learning_rate*layer.d_b\n",
    "                        \n",
    "                        WTrack_layer.append(layer.W)\n",
    "                        bTrack_layer.append(layer.b)\n",
    "                        dWTrack_layer.append(layer.d_W)\n",
    "                        dbTrack_layer.append(layer.d_b)\n",
    "                    dWTrack.append(dWTrack_layer)    \n",
    "                    dbTrack.append(dbTrack_layer)\n",
    "                    WTrack.append(WTrack_layer)    \n",
    "                    bTrack.append(bTrack_layer)  \n",
    "                    self.model.reset() #reset grads before new epoch\n",
    "            \n",
    "            #end of epoch\n",
    "            regularization=1/2*self.model.lamdba_m*np.sum([np.sum(layer.W**2) for layer in self.model.layers])\n",
    "            self.train_loss.append((self.model.loss(Y,self.model.predict(X))+regularization)/X.shape[1])\n",
    "            self.val_loss.append(self.model.loss(Yval,self.model.predict(Xval))/Xval.shape[1])\n",
    "            \n",
    "        return dWTrack,dbTrack,WTrack,bTrack\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a7f1d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce=get_loss('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65350a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_loss.<locals>.crossentropy(P, Q)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cda233",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4d78258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "\n",
    "# np.random.seed(42)#sets a seed, used for reproducability\n",
    "\n",
    "def one_hot(inarray): #converts to one hot encoding\n",
    "    outarray = np.zeros((inarray.size, inarray.max() + 1))\n",
    "    outarray[np.arange(inarray.size), inarray] = 1\n",
    "    return outarray\n",
    "\n",
    "def Preprocess(X,y):\n",
    "      \n",
    "    '''Unrolls X,y, rehsapes into column vectors, one hots y'''\n",
    "    assert(X.shape[0]==y.shape[0]),\"Inputs must contain same number of examples, stored in rows\" #checks if same dim\n",
    "    \n",
    "    X_processed=np.reshape(X,(X.shape[0],784))/255\n",
    "    X_processed=X_processed.T\n",
    "    y_processed=one_hot(y).T\n",
    "    return np.array(X_processed),y_processed\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "Xtest,ytest=Preprocess(X_test,y_test)\n",
    "\n",
    "# def tran_val_split(X,y,split=0.1):\n",
    "#     assert(X.shape[1]==y.shape[1]), \"Inputs must contain same number of examples, stored in columns\"# as vectors are now stored in cols, do check if no of elemnts are equal\n",
    "#     len_split=int(np.shape(X)[1]*split)\n",
    "#     X_val=X[:,:len_split]\n",
    "#     y_val=y[:,:len_split]\n",
    "    \n",
    "#     X_train=X[:,len_split:]\n",
    "#     y_train=y[:,len_split:]\n",
    "    \n",
    "#     return (X_train,y_train),(X_val,y_val)\n",
    "    \n",
    "        \n",
    "\n",
    "# (Xtrain,ytrain),(Xval,yval)=tran_val_split(X_train_clean,y_train_clean)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain,Xval,ytrain,yval=train_test_split(X_train,y_train,test_size=0.1)\n",
    "Xtrain,ytrain=Preprocess(Xtrain,ytrain)\n",
    "Xval,yval=Preprocess(Xval,yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "49b1818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xx,Yy=Preprocess(X_train[:16],y_train[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c9518a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7b7efac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_activation.<locals>.relu(x)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_activation('relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00c9c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47cb0bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_check(Y,Ypred):\n",
    "    return np.sum(np.argmax(Ypred,axis=0)==np.argmax(Y,axis=0))/Y.shape[1]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "ac6fa165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:59<00:00,  5.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.32212228986385 3.322690187734435\n",
      "Accuracy train 0.10001851851851852\n",
      "Accuracy val 0.09983333333333333\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt=optimizers_tester(Xtrain.shape[0],ytrain.shape[0],[32,32,32],['relu','relu','relu'],\n",
    "           loss='cross-entropy',optimizer='adam',\n",
    "           lamdba=0,batch_size=32,epochs=10,eta=1e-3)\n",
    "DW,DB,WT,BT=opt.batch_gradient_descent((Xtrain,ytrain),(Xval,yval))\n",
    "plt.plot(opt.train_loss)\n",
    "plt.plot(opt.val_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(opt.train_loss[-1],opt.val_loss[-1])\n",
    "\n",
    "print('Accuracy train',accuracy_check(opt.model.predict(Xtrain),ytrain))\n",
    "print('Accuracy val',accuracy_check(opt.model.predict(Xval),yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a39ad78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.280934698601293e-10, 3.3041923966243345e-09, 5.185318077833826e-07, 0.0029935050802200646]\n",
      "[6.573538655111388e-10, 3.0503412525824823e-09, 5.87013923845458e-07, 0.003011426298152181]\n",
      "[1.2397384666687934e-09, 4.695714209746269e-09, 5.894330591892827e-07, 0.0025692453442978944]\n",
      "[3.1296205593576663e-09, 1.1126985728215802e-08, 1.2880297088488786e-06, 0.0038807842045859775]\n",
      "[9.78472578364811e-10, 3.99349561083512e-09, 6.026448760050339e-07, 0.0033940864147862055]\n",
      "[1.3714218437133645e-09, 4.92776693679231e-09, 5.904138839343579e-07, 0.0035477505040907354]\n",
      "[5.960441394240755e-10, 2.560117885680901e-09, 4.415364181610684e-07, 0.002691973725071486]\n",
      "[1.0156548273879912e-09, 5.857717008971847e-09, 7.566690476471565e-07, 0.004957725289799827]\n",
      "[9.75833598593876e-10, 4.243284628651725e-09, 6.310929527441169e-07, 0.003564241970441968]\n",
      "[8.966993031295988e-10, 3.147226461757269e-09, 5.660790662879645e-07, 0.0024335460012537505]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print([np.linalg.norm(DW[i][j]) for j in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "f8ddd36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -6.16935278e-15\n",
      "  -6.62076883e-15 -1.95613625e-15]\n",
      " [ 0.00000000e+00 -9.20532748e-16 -8.53165089e-15 ...  1.72043430e-12\n",
      "  -1.92222086e-13 -5.53791780e-14]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  3.82855896e-16  7.38516449e-16 ...  3.95004668e-12\n",
      "   6.77007857e-13  2.01004382e-13]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   5.15561574e-16 -2.44175052e-16]]\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00 -2.30252237e-18 ... -8.74487344e-15\n",
      "  -5.98333446e-15  0.00000000e+00]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  9.05851201e-16 ...  4.67683868e-15\n",
      "   3.19994225e-15  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -4.04130499e-14\n",
      "  -3.96782672e-14  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  2.72922845e-12\n",
      "   3.08395799e-12  0.00000000e+00]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  1.57633456e-12\n",
      "   9.65356971e-13  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -2.20178800e-14\n",
      "  -7.55264490e-15  0.00000000e+00]]\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -7.34100476e-16\n",
      "  -3.67050238e-16  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  2.96681278e-15  2.96681278e-15 ...  2.63062512e-15\n",
      "   2.79871895e-15  0.00000000e+00]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00 -5.49285579e-16 -5.49285579e-16 ... -1.00550841e-14\n",
      "  -5.30218483e-15  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.94163190e-15\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -2.47340713e-15\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.07358394e-14\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n",
      "[[ 0.00000000e+00  0.00000000e+00 -5.26923462e-16 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  6.80919806e-17 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.33274981e-15 ... -1.06492769e-13\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.22518400e-14 ...  2.78737684e-13\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  8.20976966e-14\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00 -2.90553698e-15 ... -2.30784241e-13\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.63090374e-14 ...  1.00078618e-12\n",
      "   2.63473581e-13  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.24047407e-13\n",
      "  -2.71237012e-14  0.00000000e+00]]\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -4.26334385e-14\n",
      "  -1.82714737e-14  0.00000000e+00]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  6.98171719e-14\n",
      "   2.99216451e-14  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  2.35435632e-14\n",
      "   1.05232593e-14  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00 -1.95970420e-16 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  5.00324198e-16 ...  8.31285630e-12\n",
      "   7.07857398e-12  3.00984146e-12]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00 -8.06742117e-16 ...  3.13167631e-12\n",
      "   3.20256776e-12  1.61497370e-12]\n",
      " [ 0.00000000e+00  0.00000000e+00 -4.34381983e-16 ...  9.98424287e-16\n",
      "   0.00000000e+00  0.00000000e+00]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(DW[i][0])#layer x ke oopar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "99bab068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5855532923901907, 0.32801205322482196, 0.31321740938214887, 0.17772092253008126]\n",
      "[1.5855532923902018, 0.32801205322499477, 0.31321740940649456, 0.17772100627736984]\n",
      "[1.5855532923902138, 0.3280120532252221, 0.31321740943603216, 0.1777212002508608]\n",
      "[1.5855532923902602, 0.32801205322581706, 0.3132174094769973, 0.17772142240996408]\n",
      "[1.5855532923902687, 0.3280120532259992, 0.3132174094901253, 0.17772148375817715]\n",
      "[1.5855532923902833, 0.32801205322623533, 0.31321740952021665, 0.1777217709109605]\n",
      "[1.585553292390291, 0.32801205322629895, 0.3132174095092497, 0.17772160849976493]\n",
      "[1.5855532923903026, 0.32801205322661503, 0.3132174095464804, 0.17772188386212814]\n",
      "[1.5855532923903062, 0.3280120532267368, 0.3132174095587403, 0.1777220181742845]\n",
      "[1.5855532923903144, 0.328012053226874, 0.3132174095683763, 0.17772200799882318]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print([np.linalg.norm(WT[i][j]) for j in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "c4603320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.55937835e-03 -1.31277167e-02 -4.99132059e-03 ... -1.09700703e-02\n",
      "   5.25775632e-05  4.12954370e-03]\n",
      " [ 4.35701741e-04 -1.38486052e-02  2.06764415e-03 ... -6.19141847e-03\n",
      "  -7.14447690e-03  8.02009888e-03]\n",
      " [-1.35990897e-02 -1.56033615e-02  1.04689854e-02 ... -1.61525652e-02\n",
      "   6.15033489e-03  1.07782343e-02]\n",
      " ...\n",
      " [-1.49873835e-02  1.01310611e-02 -2.05770053e-03 ...  6.46410322e-03\n",
      "   3.14579299e-04 -4.07933977e-02]\n",
      " [ 7.05870455e-03 -8.29762896e-03  9.34471214e-03 ... -1.12122699e-03\n",
      "  -1.77955407e-03  2.01016185e-02]\n",
      " [-8.23999027e-03 -1.42632689e-02 -8.84242194e-03 ...  9.45867790e-03\n",
      "  -6.00583075e-03 -1.28766155e-03]]\n",
      "[[ 2.55937835e-03 -1.31277167e-02 -4.99132059e-03 ... -1.09700703e-02\n",
      "   5.25775632e-05  4.12954370e-03]\n",
      " [ 4.35701741e-04 -1.38486052e-02  2.06764415e-03 ... -6.19141847e-03\n",
      "  -7.14447690e-03  8.02009888e-03]\n",
      " [-1.35990897e-02 -1.56033615e-02  1.04689854e-02 ... -1.61525652e-02\n",
      "   6.15033489e-03  1.07782343e-02]\n",
      " ...\n",
      " [-1.49873835e-02  1.01310611e-02 -2.05770053e-03 ...  6.46410322e-03\n",
      "   3.14579299e-04 -4.07933977e-02]\n",
      " [ 7.05870455e-03 -8.29762896e-03  9.34471214e-03 ... -1.12122699e-03\n",
      "  -1.77955407e-03  2.01016185e-02]\n",
      " [-8.23999027e-03 -1.42632689e-02 -8.84242194e-03 ...  9.45867790e-03\n",
      "  -6.00583075e-03 -1.28766155e-03]]\n",
      "[[ 2.55937835e-03 -1.31277167e-02 -4.99132059e-03 ... -1.09700703e-02\n",
      "   5.25775632e-05  4.12954370e-03]\n",
      " [ 4.35701741e-04 -1.38486052e-02  2.06764415e-03 ... -6.19141847e-03\n",
      "  -7.14447690e-03  8.02009888e-03]\n",
      " [-1.35990897e-02 -1.56033615e-02  1.04689854e-02 ... -1.61525652e-02\n",
      "   6.15033489e-03  1.07782343e-02]\n",
      " ...\n",
      " [-1.49873835e-02  1.01310611e-02 -2.05770053e-03 ...  6.46410322e-03\n",
      "   3.14579299e-04 -4.07933977e-02]\n",
      " [ 7.05870455e-03 -8.29762896e-03  9.34471214e-03 ... -1.12122699e-03\n",
      "  -1.77955407e-03  2.01016185e-02]\n",
      " [-8.23999027e-03 -1.42632689e-02 -8.84242194e-03 ...  9.45867790e-03\n",
      "  -6.00583075e-03 -1.28766155e-03]]\n",
      "[[ 2.55937835e-03 -1.31277167e-02 -4.99132059e-03 ... -1.09700703e-02\n",
      "   5.25775632e-05  4.12954370e-03]\n",
      " [ 4.35701741e-04 -1.38486052e-02  2.06764415e-03 ... -6.19141847e-03\n",
      "  -7.14447690e-03  8.02009888e-03]\n",
      " [-1.35990897e-02 -1.56033615e-02  1.04689854e-02 ... -1.61525652e-02\n",
      "   6.15033489e-03  1.07782343e-02]\n",
      " ...\n",
      " [-1.49873835e-02  1.01310611e-02 -2.05770053e-03 ...  6.46410322e-03\n",
      "   3.14579299e-04 -4.07933977e-02]\n",
      " [ 7.05870455e-03 -8.29762896e-03  9.34471214e-03 ... -1.12122699e-03\n",
      "  -1.77955407e-03  2.01016185e-02]\n",
      " [-8.23999027e-03 -1.42632689e-02 -8.84242194e-03 ...  9.45867790e-03\n",
      "  -6.00583075e-03 -1.28766155e-03]]\n",
      "[[ 2.55937835e-03 -1.31277167e-02 -4.99132059e-03 ... -1.09700703e-02\n",
      "   5.25775632e-05  4.12954370e-03]\n",
      " [ 4.35701741e-04 -1.38486052e-02  2.06764415e-03 ... -6.19141847e-03\n",
      "  -7.14447690e-03  8.02009888e-03]\n",
      " [-1.35990897e-02 -1.56033615e-02  1.04689854e-02 ... -1.61525652e-02\n",
      "   6.15033489e-03  1.07782343e-02]\n",
      " ...\n",
      " [-1.49873835e-02  1.01310611e-02 -2.05770053e-03 ...  6.46410322e-03\n",
      "   3.14579299e-04 -4.07933977e-02]\n",
      " [ 7.05870455e-03 -8.29762896e-03  9.34471214e-03 ... -1.12122699e-03\n",
      "  -1.77955407e-03  2.01016185e-02]\n",
      " [-8.23999027e-03 -1.42632689e-02 -8.84242194e-03 ...  9.45867790e-03\n",
      "  -6.00583075e-03 -1.28766155e-03]]\n",
      "[[ 2.55937835e-03 -1.31277167e-02 -4.99132059e-03 ... -1.09700703e-02\n",
      "   5.25775632e-05  4.12954370e-03]\n",
      " [ 4.35701741e-04 -1.38486052e-02  2.06764415e-03 ... -6.19141847e-03\n",
      "  -7.14447690e-03  8.02009888e-03]\n",
      " [-1.35990897e-02 -1.56033615e-02  1.04689854e-02 ... -1.61525652e-02\n",
      "   6.15033489e-03  1.07782343e-02]\n",
      " ...\n",
      " [-1.49873835e-02  1.01310611e-02 -2.05770053e-03 ...  6.46410322e-03\n",
      "   3.14579299e-04 -4.07933977e-02]\n",
      " [ 7.05870455e-03 -8.29762896e-03  9.34471214e-03 ... -1.12122699e-03\n",
      "  -1.77955407e-03  2.01016185e-02]\n",
      " [-8.23999027e-03 -1.42632689e-02 -8.84242194e-03 ...  9.45867790e-03\n",
      "  -6.00583075e-03 -1.28766155e-03]]\n",
      "[[ 2.55937835e-03 -1.31277167e-02 -4.99132059e-03 ... -1.09700703e-02\n",
      "   5.25775632e-05  4.12954370e-03]\n",
      " [ 4.35701741e-04 -1.38486052e-02  2.06764415e-03 ... -6.19141847e-03\n",
      "  -7.14447690e-03  8.02009888e-03]\n",
      " [-1.35990897e-02 -1.56033615e-02  1.04689854e-02 ... -1.61525652e-02\n",
      "   6.15033489e-03  1.07782343e-02]\n",
      " ...\n",
      " [-1.49873835e-02  1.01310611e-02 -2.05770053e-03 ...  6.46410322e-03\n",
      "   3.14579299e-04 -4.07933977e-02]\n",
      " [ 7.05870455e-03 -8.29762896e-03  9.34471214e-03 ... -1.12122699e-03\n",
      "  -1.77955407e-03  2.01016185e-02]\n",
      " [-8.23999027e-03 -1.42632689e-02 -8.84242194e-03 ...  9.45867790e-03\n",
      "  -6.00583075e-03 -1.28766155e-03]]\n",
      "[[ 2.55937835e-03 -1.31277167e-02 -4.99132059e-03 ... -1.09700703e-02\n",
      "   5.25775632e-05  4.12954370e-03]\n",
      " [ 4.35701741e-04 -1.38486052e-02  2.06764415e-03 ... -6.19141847e-03\n",
      "  -7.14447690e-03  8.02009888e-03]\n",
      " [-1.35990897e-02 -1.56033615e-02  1.04689854e-02 ... -1.61525652e-02\n",
      "   6.15033489e-03  1.07782343e-02]\n",
      " ...\n",
      " [-1.49873835e-02  1.01310611e-02 -2.05770053e-03 ...  6.46410322e-03\n",
      "   3.14579299e-04 -4.07933977e-02]\n",
      " [ 7.05870455e-03 -8.29762896e-03  9.34471214e-03 ... -1.12122699e-03\n",
      "  -1.77955407e-03  2.01016185e-02]\n",
      " [-8.23999027e-03 -1.42632689e-02 -8.84242194e-03 ...  9.45867790e-03\n",
      "  -6.00583075e-03 -1.28766155e-03]]\n",
      "[[ 2.55937835e-03 -1.31277167e-02 -4.99132059e-03 ... -1.09700703e-02\n",
      "   5.25775632e-05  4.12954370e-03]\n",
      " [ 4.35701741e-04 -1.38486052e-02  2.06764415e-03 ... -6.19141847e-03\n",
      "  -7.14447690e-03  8.02009888e-03]\n",
      " [-1.35990897e-02 -1.56033615e-02  1.04689854e-02 ... -1.61525652e-02\n",
      "   6.15033489e-03  1.07782343e-02]\n",
      " ...\n",
      " [-1.49873835e-02  1.01310611e-02 -2.05770053e-03 ...  6.46410322e-03\n",
      "   3.14579299e-04 -4.07933977e-02]\n",
      " [ 7.05870455e-03 -8.29762896e-03  9.34471214e-03 ... -1.12122699e-03\n",
      "  -1.77955407e-03  2.01016185e-02]\n",
      " [-8.23999027e-03 -1.42632689e-02 -8.84242194e-03 ...  9.45867790e-03\n",
      "  -6.00583075e-03 -1.28766155e-03]]\n",
      "[[ 2.55937835e-03 -1.31277167e-02 -4.99132059e-03 ... -1.09700703e-02\n",
      "   5.25775632e-05  4.12954370e-03]\n",
      " [ 4.35701741e-04 -1.38486052e-02  2.06764415e-03 ... -6.19141847e-03\n",
      "  -7.14447690e-03  8.02009888e-03]\n",
      " [-1.35990897e-02 -1.56033615e-02  1.04689854e-02 ... -1.61525652e-02\n",
      "   6.15033489e-03  1.07782343e-02]\n",
      " ...\n",
      " [-1.49873835e-02  1.01310611e-02 -2.05770053e-03 ...  6.46410322e-03\n",
      "   3.14579299e-04 -4.07933977e-02]\n",
      " [ 7.05870455e-03 -8.29762896e-03  9.34471214e-03 ... -1.12122699e-03\n",
      "  -1.77955407e-03  2.01016185e-02]\n",
      " [-8.23999027e-03 -1.42632689e-02 -8.84242194e-03 ...  9.45867790e-03\n",
      "  -6.00583075e-03 -1.28766155e-03]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(WT[i][0])#layer x ke oopar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e04a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "WT[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176129f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=optimizers(Xtrain.shape[0],ytrain.shape[0],[32,32,32],['sigmoid','sigmoid','sigmoid'],\n",
    "           loss='cross-entropy',optimizer='adam',\n",
    "           lamdba=0,batch_size=32,epochs=10,eta=1e-5)\n",
    "opt.Momentum((Xtrain,ytrain),(Xval,yval))\n",
    "plt.plot(opt.train_loss)\n",
    "plt.plot(opt.val_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(opt.train_loss[-1],opt.val_loss[-1])\n",
    "\n",
    "print('Accuracy train',accuracy_check(opt.model.predict(Xtrain),ytrain))\n",
    "print('Accuracy val',accuracy_check(opt.model.predict(Xval),yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963810f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=optimizers(Xtrain.shape[0],ytrain.shape[0],[32,32,32],['sigmoid','sigmoid','sigmoid'],\n",
    "           loss='cross-entropy',optimizer='adam',\n",
    "           lamdba=0,batch_size=1,epochs=10,eta=1e-5)\n",
    "opt.Momentum((Xtrain,ytrain),(Xval,yval))\n",
    "plt.plot(opt.train_loss)\n",
    "plt.plot(opt.val_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(opt.train_loss[-1],opt.val_loss[-1])\n",
    "\n",
    "print('Accuracy train',accuracy_check(opt.model.predict(Xtrain),ytrain))\n",
    "print('Accuracy val',accuracy_check(opt.model.predict(Xval),yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7314317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=optimizers(Xtrain.shape[0],ytrain.shape[0],[32,32,32],['sigmoid','sigmoid','sigmoid'],\n",
    "           loss='cross-entropy',optimizer='adam',\n",
    "           lamdba=0,batch_size=32,epochs=10,eta=1e-5)\n",
    "opt.Momentum((Xtrain,ytrain),(Xval,yval))\n",
    "plt.plot(opt.train_loss)\n",
    "plt.plot(opt.val_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(opt.train_loss[-1],opt.val_loss[-1])\n",
    "\n",
    "print('Accuracy train',accuracy_check(opt.model.predict(Xtrain),ytrain))\n",
    "print('Accuracy val',accuracy_check(opt.model.predict(Xval),yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451426af",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.train_loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b62fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=optimizers(Xtrain.shape[0],ytrain.shape[0],[32,32,32],['sigmoid','sigmoid','sigmoid'],\n",
    "           loss='cross-entropy',optimizer='adam',\n",
    "           lamdba=0,batch_size=32,epochs=10,eta=1e-2)\n",
    "opt.Adam((Xtrain,ytrain),(Xval,yval))\n",
    "plt.plot(opt.train_loss)\n",
    "plt.plot(opt.val_loss)\n",
    "\n",
    "\n",
    "print(opt.train_loss[-1],opt.val_loss[-1])\n",
    "print('Accuracy train',accuracy_check(opt.model.predict(Xtrain),ytrain))\n",
    "print('Accuracy val',accuracy_check(opt.model.predict(Xval),yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8910a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=optimizers(Xtrain.shape[0],ytrain.shape[0],[32,32,32],['sigmoid','sigmoid','sigmoid'],\n",
    "           loss='cross-entropy',optimizer='adam',\n",
    "           lamdba=0,batch_size=32,epochs=10,eta=1e-1)\n",
    "opt.Adam((Xtrain,ytrain),(Xval,yval))\n",
    "plt.plot(opt.train_loss)\n",
    "plt.plot(opt.val_loss)\n",
    "\n",
    "\n",
    "print(opt.train_loss[-1],opt.val_loss[-1])\n",
    "print('Accuracy train',accuracy_check(opt.model.predict(Xtrain),ytrain))\n",
    "print('Accuracy val',accuracy_check(opt.model.predict(Xval),yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=optimizers(Xtrain.shape[0],ytrain.shape[0],[32,32,32],['sigmoid','sigmoid','sigmoid'],\n",
    "           loss='cross-entropy',optimizer='adam',\n",
    "           lamdba=0,batch_size=32,epochs=10,eta=1e-3)\n",
    "opt.Adam((Xtrain,ytrain),(Xval,yval))\n",
    "plt.plot(opt.train_loss)\n",
    "plt.plot(opt.val_loss)\n",
    "\n",
    "\n",
    "print(opt.train_loss[-1],opt.val_loss[-1])\n",
    "print('Accuracy train',accuracy_check(opt.model.predict(Xtrain),ytrain))\n",
    "print('Accuracy val',accuracy_check(opt.model.predict(Xval),yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=optimizers(Xtrain.shape[0],ytrain.shape[0],[32,32,32],['sigmoid','sigmoid','sigmoid'],\n",
    "           loss='cross-entropy',optimizer='adam',\n",
    "           lamdba=0,batch_size=32,epochs=10,eta=1e-3)\n",
    "opt.NAG((Xtrain,ytrain),(Xval,yval))\n",
    "plt.plot(opt.train_loss)\n",
    "plt.plot(opt.val_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(opt.train_loss[-1],opt.val_loss[-1])\n",
    "\n",
    "print('Accuracy train',accuracy_check(opt.model.predict(Xtrain),ytrain))\n",
    "print('Accuracy val',accuracy_check(opt.model.predict(Xval),yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72872e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=optimizers(Xtrain.shape[0],ytrain.shape[0],[32,32,32],['sigmoid','sigmoid','sigmoid'],\n",
    "           loss='cross-entropy',optimizer='adam',\n",
    "           lamdba=0,batch_size=32,epochs=10,eta=1e-3)\n",
    "opt.rmsprop((Xtrain,ytrain),(Xval,yval))\n",
    "plt.plot(opt.train_loss)\n",
    "plt.plot(opt.val_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(opt.train_loss[-1],opt.val_loss[-1])\n",
    "\n",
    "print('Accuracy train',accuracy_check(opt.model.predict(Xtrain),ytrain))\n",
    "print('Accuracy val',accuracy_check(opt.model.predict(Xval),yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29461f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=optimizers(Xtrain.shape[0],ytrain.shape[0],[64,64,64],['tanh','tanh','tanh'],\n",
    "           loss='cross-entropy',optimizer='adam',\n",
    "           lamdba=0,batch_size=64,epochs=20,eta=1e-3)\n",
    "opt.NAdam((Xtrain,ytrain),(Xval,yval))\n",
    "plt.plot(opt.train_loss)\n",
    "plt.plot(opt.val_loss)\n",
    "\n",
    "\n",
    "print(opt.train_loss[-1],opt.val_loss[-1])\n",
    "print('Accuracy train',accuracy_check(opt.model.predict(Xtrain),ytrain))\n",
    "print('Accuracy val',accuracy_check(opt.model.predict(Xval),yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53017724",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_activation('relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c7da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain[:,[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a3cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt1=optimizers_beta(Xtrain.shape[0],ytrain.shape[0],[32,32,32],['sigmoid','sigmoid','sigmoid'],\n",
    "           loss='cross-entropy',optimizer='adam',\n",
    "           lamdba=0,batch_size=32,epochs=10,eta=1)\n",
    "opt1.Momentum(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d0452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt1.model.predict(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3805a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fe4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b066dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de5afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7031b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.model.predict(Xtrain[:,[1\n",
    "                           ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84457c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain[:,[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3203cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=optimizers(Xtrain.shape[0],ytrain.shape[0],[32,32,32],['sigmoid','sigmoid','sigmoid'],\n",
    "           loss='cross-entropy',optimizer='adam',\n",
    "           lamdba=0,batch_size=1,epochs=10,eta=1e-3)\n",
    "opt.Adam((Xtrain,ytrain),(Xval,yval))\n",
    "plt.plot(opt.train_loss)\n",
    "plt.plot(opt.val_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(opt.train_loss[-1],opt.val_loss[-1])\n",
    "\n",
    "print('Accuracy train',accuracy_check(opt.model.predict(Xtrain),ytrain))\n",
    "print('Accuracy val',accuracy_check(opt.model.predict(Xval),yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16a9d641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain[:,[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a04b66b",
   "metadata": {},
   "source": [
    "# Shapap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "867d2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "7745ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapap=optimizers(Xtrain.shape[0],ytrain.shape[0],[5,5,5],['relu','relu','relu'],\n",
    "           loss='cross-entropy',optimizer='adam',\n",
    "           lamdba=0,batch_size=16,epochs=10,eta=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "f4fee01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ypred=shapap.model.forward(Xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "668e4712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.57497958e-01, -3.73838419e-01, -1.38506188e-01,\n",
       "        -1.84785739e-01, -3.49965304e-01, -2.15167615e-01,\n",
       "        -1.64455218e-01, -4.24412739e-01, -9.53258543e-02,\n",
       "        -1.13612899e-01, -3.76787240e-01, -4.42210553e-01,\n",
       "        -1.70793795e-01, -9.23270012e-02, -1.09692716e-01,\n",
       "        -3.38804149e-01],\n",
       "       [ 9.28711497e-02,  2.76653672e-02, -2.48451862e-02,\n",
       "        -6.35902708e-04, -5.26088578e-02,  1.16236986e-01,\n",
       "         1.03458197e-01,  6.67683267e-02, -1.22379863e-02,\n",
       "         5.01254676e-02,  9.59240407e-02,  1.94725573e-01,\n",
       "         5.26731299e-02, -2.18407140e-02,  5.78365041e-02,\n",
       "         1.27124157e-01],\n",
       "       [-2.30183122e-01, -1.67949529e-01, -9.24765413e-02,\n",
       "        -1.24965639e-01, -1.30870415e-01, -1.83593745e-01,\n",
       "        -1.99246114e-02, -1.77127657e-01,  4.76844844e-02,\n",
       "         2.14324812e-03, -1.20095288e-01, -1.30267760e-01,\n",
       "        -1.01596768e-01, -2.99721894e-02,  6.00506298e-02,\n",
       "        -1.71606325e-01],\n",
       "       [-1.37646553e-01, -7.86085075e-02, -4.37219672e-02,\n",
       "        -8.08216354e-02, -1.10066102e-01, -3.06108091e-01,\n",
       "        -8.52514824e-02, -2.81490798e-01, -1.40624692e-01,\n",
       "        -1.08230284e-01, -1.00748784e-01, -3.10227172e-01,\n",
       "        -1.03642791e-01, -9.31933692e-02, -7.22793437e-02,\n",
       "        -2.00695092e-01],\n",
       "       [ 3.12311023e-02, -2.60083281e-01, -1.02448370e-01,\n",
       "        -2.09102290e-01, -3.13555746e-01, -1.06254853e-01,\n",
       "         4.15666249e-02, -1.40187704e-01, -2.63256995e-02,\n",
       "        -4.76766970e-03, -2.63460172e-01, -1.74545835e-01,\n",
       "         3.41224110e-04,  2.34525329e-02,  1.80567690e-02,\n",
       "         2.67029814e-02]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapap.model.layers[0].a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "71f6a161",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (10,1) doesn't match the broadcast shape (10,16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [281], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mshapap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mYy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshapap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [78], line 52\u001b[0m, in \u001b[0;36mModel.backward\u001b[0;34m(self, x, y, y_pred)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m): \u001b[38;5;66;03m#goes from L->2, for l=1 we do outside\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \n\u001b[1;32m     49\u001b[0m     \n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m#compute gradient wrt parameters\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[idx]\u001b[38;5;241m.\u001b[39md_W\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[idx]\u001b[38;5;241m.\u001b[39md_a,np\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[idx\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mh))\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlamdba_m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[idx]\u001b[38;5;241m.\u001b[39mW\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[idx]\u001b[38;5;241m.\u001b[39md_b\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[idx]\u001b[38;5;241m.\u001b[39md_a\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m#compute gradient wrt layer below -- will help in next layer iter\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[idx\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39md_h\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmatmul(np\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[idx]\u001b[38;5;241m.\u001b[39mW),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[idx]\u001b[38;5;241m.\u001b[39md_a)\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (10,1) doesn't match the broadcast shape (10,16)"
     ]
    }
   ],
   "source": [
    "shapap.model.backward(Xx,Yy,shapap.model.forward(Xx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "ec973324",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Xx.shape[1]):\n",
    "    shapap.model.backward(Xx[:,[i]],Yy[:,[i]],shapap.model.forward(Xx[:,[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "1b796f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -4.03196344e-17,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapap.model.layers[0].d_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "598eacd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  2.67737280e-07,\n",
       "         1.72480236e-07,  1.15345572e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  2.67737265e-07,\n",
       "         1.72480226e-07,  7.22084530e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  2.67737256e-07,\n",
       "         1.72480220e-07, -1.76345528e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  2.67737260e-07,\n",
       "         1.72480223e-07,  7.22084414e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  2.67737264e-07,\n",
       "         1.72480225e-07,  7.22084511e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  2.67737274e-07,\n",
       "         1.72480232e-07,  5.85629429e-07],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  2.67737274e-07,\n",
       "         1.72480232e-07,  7.22084353e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00, -2.40963539e-06,\n",
       "        -1.55232204e-06, -8.33651396e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  2.67737259e-07,\n",
       "         1.72480222e-07,  7.22084401e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  2.67737258e-07,\n",
       "         1.72480222e-07, -2.77433380e-05]])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapap.model.layers[3].d_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaedd347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
