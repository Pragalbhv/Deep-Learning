{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caab2aca-d1a8-4839-a5eb-db43e5e6542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a7ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "238d8eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 12:01:09.569527: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 12:01:09.642902: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-14 12:01:09.659879: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-14 12:01:09.912492: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-14 12:01:09.912536: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-14 12:01:09.912539: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d789c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc49ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90955b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab7d1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)#sets a seed, used for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11b33e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(inarray): #converts to one hot encoding\n",
    "    outarray = np.zeros((inarray.size, inarray.max() + 1))\n",
    "    outarray[np.arange(inarray.size), inarray] = 1\n",
    "    return outarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4892ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess(X,y):\n",
    "      \n",
    "    '''Unrolls X,y, rehsapes into column vectors, one hots y'''\n",
    "    assert(X.shape[0]==y.shape[0]),\"Inputs must contain same number of examples, stored in rows\" #checks if same dim\n",
    "    X_processed=[]\n",
    "    y_processed=[]\n",
    "    \n",
    "    for i in range(np.shape(X)[0]):\n",
    "        X_processed.append(X_train[i].ravel())\n",
    "    y_processed=one_hot(y_train).T\n",
    "    return np.array(X_processed).T,y_processed\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00f95823",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean,y_train_clean=Preprocess(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "678e5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest,ytest=Preprocess(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0f18031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tran_val_split(X,y,split=0.1):\n",
    "    assert(X.shape[1]==y.shape[1]), \"Inputs must contain same number of examples, stored in columns\"# as vectors are now stored in cols, do check if no of elemnts are equal\n",
    "    len_split=int(np.shape(X)[1]*split)\n",
    "    np.random.shuffle(X)\n",
    "    np.random.shuffle(y)\n",
    "    X_val=X[:,:len_split]\n",
    "    y_val=y[:,:len_split]\n",
    "    \n",
    "    X_train=X[:,len_split:]\n",
    "    y_train=y[:,len_split:]\n",
    "    \n",
    "    return (X_train,y_train),(X_val,y_val)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9474fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Xtrain,ytrain),(Xval,yval)=tran_val_split(X_train_clean,y_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0464a583-6e66-4d4b-a9fe-d5c9529039a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    return np.where(x >= 0, \n",
    "                    1 / (1 + np.exp(-x)), \n",
    "                    np.exp(x) / (1 + np.exp(x)))\n",
    "def softmax(x):\n",
    "\n",
    "    z=x-np.max(x,axis=0) #doing this for numerical stability, prevents over/undeflow\n",
    "    return np.exp(z)/np.sum(np.exp(z),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1760ea39-c325-4e77-90a4-2bfd4fb3c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(activation):#maybe getderivatives here iteself ?\n",
    "    def sigmoid(x):\n",
    "        return np.where(x >= 0, \n",
    "                        1 / (1 + np.exp(-x)), \n",
    "                        np.exp(x) / (1 + np.exp(x)))\n",
    "    def softmax(x):\n",
    "        z=x-np.max(x,axis=0)\n",
    "        return np.exp(z)/np.sum(np.exp(z),axis=0)\n",
    "    if activation=='sigmoid':\n",
    "        return sigmoid\n",
    "    elif activation=='softmax':\n",
    "        return softmax\n",
    "    elif activation== 'tanh':\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12f6598e-effc-4cda-b1b6-7aa640aad858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_derivative(activation):#maybe getderivatives here iteself ?\n",
    "    def sigmoid_d(x):\n",
    "        sig= np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "        return sig*(1-sig)\n",
    "    def softmax_d(x):\n",
    "        z=x-np.max(x,axis=0)\n",
    "        soft=np.exp(z)/np.sum(np.exp(z),axis=0)\n",
    "        return soft*(1-soft)\n",
    "    if activation=='sigmoid':\n",
    "        return sigmoid_d\n",
    "    elif activation=='softmax':\n",
    "        '''\n",
    "        need to think more, not required for backprop as we look directly at dL/da_l\n",
    "        But still, for the sake of completeness, and if user wants softmax in the middle?\n",
    "        d S(x_i) /d x_j= S(x_i)*(kronecker delta_i,j -S(x_j))\n",
    "        But we care about only dh_k,j/da_k,j So no need to implement d S(x_i) /d x_j\n",
    "        d S(x_i) /d x_i should suffice\n",
    "        so we get array of [ d S(x_1) /d x_1, d S(x_2) /d x_2, ....]\n",
    "        \n",
    "        For MSE loss after softmax, we need cross terms...\n",
    "        '''\n",
    "        \n",
    "        return softmax_d\n",
    "    elif activation=='tanh':\n",
    "        return 1-tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a0d0152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(loss):\n",
    "    \n",
    "   \n",
    "    \n",
    "        \n",
    "    def crossentropy(P,Q):\n",
    "        assert(P.shape==Q.shape), \"Inputs must be of same shape\"\n",
    "\n",
    "        return np.sum([-np.dot(P[:,i],np.log2(Q[:,i])) for i in range(P.shape[1])])\n",
    "    def SE(P,Q):\n",
    "        assert(P.shape==Q.shape), \"Inputs must be of same shape\"\n",
    "\n",
    "        return np.square(P-Q)\n",
    "    \n",
    "    if loss==\"cross-entropy\":\n",
    "        return crossentropy\n",
    "    return SE\n",
    "    \n",
    "    \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f55729cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_derivative(loss):\n",
    "    def SE_d(y_in,y_pred_in):\n",
    "        '''\n",
    "        derivative of MSE after softmax is used to get probabs from a_L:\n",
    "        We need indicator because the all terms of y_true are required unlike cross-entropy where only y_pred[l] is required\n",
    "        Thus transforming the stacked indicator to y_true, not here...\n",
    "        \n",
    "        '''\n",
    "\n",
    "        def indicator(i,j):\n",
    "                if i==j:\n",
    "                    return 1\n",
    "                return 0\n",
    "\n",
    "\n",
    "        assert(y_in.shape[0]==y_pred_in.shape[0]),\"Inputs must contain same number of examples\"\n",
    "\n",
    "        y=y_in.ravel()\n",
    "        y_pred=y_pred_in.ravel()\n",
    "\n",
    "\n",
    "        return np.array([\n",
    "            [2*np.sum([(y_pred[i]-y[i])*y[i]*(indicator(i,j) - y_pred[j]) for i in range(y.shape[0])])]\n",
    "            for j in range(len(y))\n",
    "        ])    \n",
    "   \n",
    "    \n",
    "        \n",
    "    def crossentropy_d(y,y_pred):\n",
    "        \n",
    "\n",
    "        return -(y-y_pred)\n",
    "    \n",
    "    \n",
    "    if loss==\"cross-entropy\":\n",
    "        return crossentropy_d\n",
    "    return SE_d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a66ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def SE_d(y_in,y_pred_in):\n",
    "    \n",
    "#     def indicator(i,j):\n",
    "#             if i==j:\n",
    "#                 return 1\n",
    "#             return 0\n",
    "        \n",
    "        \n",
    "#     assert(y_in.shape[0]==y_pred_in.shape[0]),\"Inputs must contain same number of examples\"\n",
    "\n",
    "#     y=y_in.ravel()\n",
    "#     y_pred=y_pred_in.ravel()\n",
    "\n",
    "\n",
    "#     return np.array([\n",
    "#         [2*np.sum([(y_pred[i]-y[i])*y[i]*(indicator(i,j) - y_pred[j]) for i in range(y.shape[0])])]\n",
    "#         for j in range(len(y))\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83131eae-4c3a-4139-b5c6-65bc10c558c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self,input_size,output_size,activation='sigmoid'):\n",
    "            \n",
    "        ''' \n",
    "        output size number of neurons i\n",
    "        input size j\n",
    "        \n",
    "        '''\n",
    "        scale=0.01\n",
    "        self.W=np.random.randn(output_size,input_size)*scale #size ixj\n",
    "        self.b=np.random.randn(output_size,1)*scale            #size i\n",
    "        self.a=np.random.randn(output_size,1)*scale            #size i\n",
    "        self.h=np.random.randn(output_size,1)*scale            #size i\n",
    "        self.g=get_activation(activation)\n",
    "        \n",
    "        self.d_a=np.zeros((output_size,1))\n",
    "        self.d_h=np.zeros((output_size,1))\n",
    "        self.d_W=np.zeros((output_size,input_size))\n",
    "        self.d_b=np.zeros((output_size,1))\n",
    "        self.d_g=get_activation_derivative(activation)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.a=self.b+np.matmul(self.W,inputs)\n",
    "        self.h=self.g(self.a)\n",
    "        return self.h\n",
    "    def reset(self):\n",
    "        self.d_a=np.zeros(np.shape(self.d_a))\n",
    "        self.d_h=np.zeros(np.shape(self.d_h))\n",
    "        self.d_W=np.zeros(np.shape(self.d_W))\n",
    "        self.d_b=np.zeros(np.shape(self.d_b))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58b39b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.eye(4)\n",
    "np.linalg.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b45ca6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(x.ravel(),ord=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cd88683-59f7-48b6-8f02-a61382ecc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,X_size,Y_size,hidden_layer_sizes=[4],hidden_layer_activations=['sigmoid'],loss='cross-entropy',lamdba_m=0):\n",
    "        '''\n",
    "        '''   \n",
    "        \n",
    "        self.input_size=X_size\n",
    "        self.output_size=Y_size\n",
    "        self.hidden_layer_sizes=hidden_layer_sizes\n",
    "        self.layers=[]\n",
    "        \n",
    "        prev_size=self.input_size\n",
    "    \n",
    "        for size,activation in zip(hidden_layer_sizes,hidden_layer_activations):\n",
    "            self.layers.append(layer(prev_size,size,activation))\n",
    "            prev_size=size\n",
    "        self.layers.append(layer(size,self.output_size,'softmax'))\n",
    "        \n",
    "        self.loss=get_loss(loss)#without regularization term\n",
    "        self.loss_d=get_loss_derivative(loss)\n",
    "        self.lamdba_m=lamdba_m #we shall pass lambda/m to this, where m is patch size\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output=x\n",
    "        # print(output.shape)\n",
    "        for layer in  self.layers:\n",
    "            # print('W',layer.W.shape)\n",
    "            output=layer.forward(output)\n",
    "            # print(output.shape)   \n",
    "        return output\n",
    "    \n",
    "    def reset(self):\n",
    "        for layer in  self.layers:\n",
    "            # resets the dWs\n",
    "            layer.reset()\n",
    "    \n",
    " \n",
    "        \n",
    "    \n",
    "          \n",
    "            \n",
    "    def backward(self,x,y,y_pred):\n",
    "        # self.layers[-1].d_h is not needed as d_h is used to calculate d_a and self.layers[-1].h is softmax\n",
    "        self.layers[-1].d_a=self.loss_d(y,y_pred)\n",
    "            \n",
    "        \n",
    "        \n",
    "        for idx in range(len(self.layers)-1,0,-1): #goes from L->2, for l=1 we do outside\n",
    "            \n",
    "            \n",
    "            #compute gradient wrt parameters\n",
    "            self.layers[idx].d_W+=np.matmul(self.layers[idx].d_a,np.transpose(self.layers[idx-1].h))+self.lamdba_m*self.layers[idx].W\n",
    "            self.layers[idx].d_b+=self.layers[idx].d_a\n",
    "            \n",
    "            #compute gradient wrt layer below -- will help in next layer iter\n",
    "            self.layers[idx-1].d_h=np.matmul(np.transpose(self.layers[idx].W),self.layers[idx].d_a)\n",
    "            \n",
    "            #compute gradient -- element wise multiplivation, derivative of the activation function of layer idx-1\n",
    "            self.layers[idx-1].d_a=self.layers[idx-1].d_h*self.layers[idx-1].d_g(self.layers[idx-1].a)\n",
    "                        \n",
    "        self.layers[0].d_W+=np.matmul(self.layers[0].d_a,np.transpose(x))+self.lamdba_m*self.layers[0].W\n",
    "        self.layers[0].d_b+=self.layers[0].d_a \n",
    "        \n",
    "    def predict(self,Xtest):\n",
    "        preds=[]\n",
    "        for i in range(Xtest.shape[1]):\n",
    "            preds.append(self.forward(Xtest[:,[i]]))\n",
    "        \n",
    "        ytest_pred=np.hstack(preds)\n",
    "        return ytest_pred\n",
    "    \n",
    "        \n",
    "        \n",
    "                    \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5582733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_model=Model(784,10,[32],['sigmoid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "481ebecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_model.predict(Xtest).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a84be20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizers_legacy(Model):\n",
    "    def __init__(self,X_size,Y_size,hidden_layer_sizes=[4],hidden_layer_activations=['sigmoid']):\n",
    "        super().__init__(X_size,Y_size,hidden_layer_sizes,hidden_layer_activations)\n",
    "\n",
    "    def batch_gradient_descent(self,X,Y,eta=1,batch_size=1,max_iters=1000):\n",
    "        \n",
    "        '''\n",
    "        Mini-Batch Gradient Descent\n",
    "        at batchsize=1, behaves like sgd, batchsize=np.shape(X), behaves as gd\n",
    "        eta is the learning rate\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "                #update if the number of points seen==batch size, or if data ends\n",
    "                if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                    for layer in self.layers:\n",
    "                        layer.W=layer.W-eta*layer.d_W\n",
    "                        layer.b=layer.b-eta*layer.d_b\n",
    "                    self.reset() #reset grads before new epoch\n",
    "            \n",
    "            \n",
    "    def stochastic_gradient_descent(self,X,Y,eta=1,max_iters=1000):\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        stochastic Gradient Descent\n",
    "        '''\n",
    "                \n",
    "        \n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                \n",
    "                #update\n",
    "                for layer in self.layers:\n",
    "                    layer.W=layer.W-eta*layer.d_W\n",
    "                    layer.b=layer.b-eta*layer.d_b\n",
    "                self.reset() #reset grads before new update\n",
    "                    \n",
    "            \n",
    "        \n",
    "        \n",
    "    def Momentum_based(self,X,Y,eta=1,beta=0.9,batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        u_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        u_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    u_W[i]=beta*u_W[i]+layer.d_W\n",
    "                    u_b[i]=beta*u_b[i]+layer.d_b\n",
    "                    layer.W=layer.W-eta*u_W[i]\n",
    "                    layer.b=layer.b-eta*u_b[i]\n",
    "                self.reset() #reset grads before new update\n",
    "                \n",
    "\n",
    "\n",
    "    def rmsprop(self,X,Y,eta=1,beta=0.9,batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    v_W[i]=beta*v_W[i]+(1-beta)*layer.d_W**2\n",
    "                    v_b[i]=beta*v_b[i]+(1-beta)*layer.d_b**2\n",
    "                    layer.W=layer.W-(eta/np.sqrt(v_W[i]+epsilon))*layer.d_W\n",
    "                    layer.b=layer.b-(eta/np.sqrt(v_b[i]+epsilon))*layer.d_b\n",
    "                self.reset() #reset grads before new epoch\n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "    def adam(self,X,Y,eta=1,beta1=0.9, beta2=0.999, batch_size=100,max_iters=1000):\n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    #updating momentum, velocity\n",
    "                    m_W[i]=beta1*m_W[i]+(1-beta1)*layer.d_W\n",
    "                    m_b[i]=beta1*m_b[i]+(1-beta1)*layer.d_b\n",
    "                    \n",
    "                    v_W[i]=beta2*v_W[i]+(1-beta2)*layer.d_W**2\n",
    "                    v_b[i]=beta2*v_b[i]+(1-beta2)*layer.d_b**2\n",
    "                    \n",
    "                    m_W_hat=m_W[i]/(1-np.power(beta1,t+1))\n",
    "                    m_b_hat=m_b[i]/(1-np.power(beta1,t+1))\n",
    "                    v_W_hat=v_W[i]/(1-np.power(beta2,t+1))\n",
    "                    v_b_hat=v_b[i]/(1-np.power(beta2,t+1))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    layer.W=layer.W-(eta*m_W_hat)/(np.sqrt(v_W_hat)+epsilon)\n",
    "                    layer.b=layer.b-(eta*m_b_hat)/(np.sqrt(v_b_hat)+epsilon)\n",
    "                self.reset() #reset grads before new epoch\n",
    "    \n",
    "    def NAG(self,X,Y,eta=1,beta=0.9,batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    m_W[i]=beta*m_W[i]+eta*layer.d_W\n",
    "                    m_b[i]=beta*m_b[i]+eta*layer.d_b\n",
    "\n",
    "                    \n",
    "                    layer.W=layer.W-(beta*m_W[i]+eta*layer.d_W[i])\n",
    "                    layer.b=layer.b-(beta*m_b[i]+eta*layer.d_b[i])\n",
    "                self.reset() #reset grads before new epoch\n",
    "                \n",
    "                \n",
    "    \n",
    "    def NAdam(self,X,Y,eta=1,beta1=0.9, beta2=0.999, batch_size=100,max_iters=1000):\n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    #updating momentum, velocity\n",
    "                    m_W[i]=beta1*m_W[i]+(1-beta1)*layer.d_W\n",
    "                    m_b[i]=beta1*m_b[i]+(1-beta1)*layer.d_b\n",
    "                    \n",
    "                    v_W[i]=beta2*v_W[i]+(1-beta2)*layer.d_W**2\n",
    "                    v_b[i]=beta2*v_b[i]+(1-beta2)*layer.d_b**2\n",
    "                    \n",
    "                    m_W_hat=m_W[i]/(1-np.power(beta1,t+1))\n",
    "                    m_b_hat=m_b[i]/(1-np.power(beta1,t+1))\n",
    "                    v_W_hat=v_W[i]/(1-np.power(beta2,t+1))\n",
    "                    v_b_hat=v_b[i]/(1-np.power(beta2,t+1))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    layer.W=layer.W-(eta/(np.sqrt(v_W_hat)+epsilon))*\\\n",
    "                    (beta1*m_W_hat+((1-beta1)/(1-np.power(beta1,t+1)))*layer.d_W)\n",
    "                    layer.b=layer.b-(eta/(np.sqrt(v_b_hat)+epsilon))*\\\n",
    "                    (beta1*m_b_hat+((1-beta1)/(1-np.power(beta1,t+1)))*layer.d_b)\n",
    "                    \n",
    "                self.reset() #reset grads before new epoch\n",
    "            \n",
    "\n",
    "                \n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9019f0db-4847-4420-80c0-a93ab6c65254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizers(Model):\n",
    "    def __init__(self,X_size,Y_size,hidden_layer_sizes=[4],hidden_layer_activations=['sigmoid'],loss='cross-entropy',optimizer='adam',lamdba=0,batch_size=5,epochs=10,eta=1e-3):\n",
    "        self.batch_size=batch_size\n",
    "        self.epochs=epochs\n",
    "        self.train_loss=0\n",
    "        self.val_loss=0\n",
    "        self.model=Model(X_size,Y_size,hidden_layer_sizes,hidden_layer_activations,loss,lamdba/self.batch_size)\n",
    "        self.learning_rate=eta\n",
    "        self.optimizer=optimizer\n",
    "        \n",
    "\n",
    "    def batch_gradient_descent(self,traindat,testdat,verbose=False):\n",
    "        \n",
    "        '''\n",
    "        Mini-Batch Gradient Descent\n",
    "        at batchsize=1, behaves like sgd, batchsize=np.shape(X), behaves as gd\n",
    "        eta is the learning rate\n",
    "        '''\n",
    "        X,Y=traindat\n",
    "        \n",
    "        Xval,Yval=testdat\n",
    "        eta=self.learning_rate\n",
    "        \n",
    "\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "                #update if the number of points seen==batch size, or if data ends\n",
    "                if number_points_seen%self.batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                    for layer in self.model.layers:\n",
    "                        layer.W=layer.W-eta*layer.d_W\n",
    "                        layer.b=layer.b-eta*layer.d_b\n",
    "                    self.model.reset() #reset grads before new epoch\n",
    "            \n",
    "            #end of epoch\n",
    "            regularization=1/2*self.model.lamdba_m*np.sum([np.sum(layer.W**2) for layer in self.model.layers])\n",
    "            self.train_loss=self.model.loss(Y,self.model.predict(X))\n",
    "            self.val_loss=self.model.loss(Yval,self.model.predict(Xval))\n",
    "            \n",
    "            \n",
    "    def stochastic_gradient_descent(self,X,Y,eta=1e-3,max_iters=10):\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        stochastic Gradient Descent\n",
    "        '''\n",
    "                \n",
    "        \n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                \n",
    "                #update\n",
    "                for layer in self.layers:\n",
    "                    layer.W=layer.W-eta*layer.d_W\n",
    "                    layer.b=layer.b-eta*layer.d_b\n",
    "                self.reset() #reset grads before new update\n",
    "                    \n",
    "            \n",
    "        \n",
    "        \n",
    "    def Momentum(self,X,Y,eta=1e-3,beta=0.9,batch_size=100,max_iters=10):\n",
    "        ''''''\n",
    "        u_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        u_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    u_W[i]=beta*u_W[i]+layer.d_W\n",
    "                    u_b[i]=beta*u_b[i]+layer.d_b\n",
    "                    layer.W=layer.W-eta*u_W[i]\n",
    "                    layer.b=layer.b-eta*u_b[i]\n",
    "                self.reset() #reset grads before new update\n",
    "                \n",
    "\n",
    "\n",
    "    def rmsprop(self,X,Y,eta=1e-3,beta=0.9,batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    v_W[i]=beta*v_W[i]+(1-beta)*layer.d_W**2\n",
    "                    v_b[i]=beta*v_b[i]+(1-beta)*layer.d_b**2\n",
    "                    layer.W=layer.W-(eta/np.sqrt(v_W[i]+epsilon))*layer.d_W\n",
    "                    layer.b=layer.b-(eta/np.sqrt(v_b[i]+epsilon))*layer.d_b\n",
    "                self.reset() #reset grads before new epoch\n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "    def Adam(self,X,Y,eta=1e-3,beta1=0.9, beta2=0.999, batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    #updating momentum, velocity\n",
    "                    m_W[i]=beta1*m_W[i]+(1-beta1)*layer.d_W\n",
    "                    m_b[i]=beta1*m_b[i]+(1-beta1)*layer.d_b\n",
    "                    \n",
    "                    v_W[i]=beta2*v_W[i]+(1-beta2)*layer.d_W**2\n",
    "                    v_b[i]=beta2*v_b[i]+(1-beta2)*layer.d_b**2\n",
    "                    \n",
    "                    m_W_hat=m_W[i]/(1-np.power(beta1,t+1))\n",
    "                    m_b_hat=m_b[i]/(1-np.power(beta1,t+1))\n",
    "                    v_W_hat=v_W[i]/(1-np.power(beta2,t+1))\n",
    "                    v_b_hat=v_b[i]/(1-np.power(beta2,t+1))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    layer.W=layer.W-(eta*m_W_hat)/(np.sqrt(v_W_hat)+epsilon)\n",
    "                    layer.b=layer.b-(eta*m_b_hat)/(np.sqrt(v_b_hat)+epsilon)\n",
    "                self.reset() #reset grads before new epoch\n",
    "    \n",
    "    def NAG(self,X,Y,eta=1e-3,beta=0.9,batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    m_W[i]=beta*m_W[i]+eta*layer.d_W\n",
    "                    m_b[i]=beta*m_b[i]+eta*layer.d_b\n",
    "\n",
    "                    \n",
    "                    layer.W=layer.W-(beta*m_W[i]+eta*layer.d_W[i])\n",
    "                    layer.b=layer.b-(beta*m_b[i]+eta*layer.d_b[i])\n",
    "                self.reset() #reset grads before new epoch\n",
    "                \n",
    "                \n",
    "    \n",
    "    def NAdam(self,X,Y,eta=1e-3,beta1=0.9, beta2=0.999, batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    #updating momentum, velocity\n",
    "                    m_W[i]=beta1*m_W[i]+(1-beta1)*layer.d_W\n",
    "                    m_b[i]=beta1*m_b[i]+(1-beta1)*layer.d_b\n",
    "                    \n",
    "                    v_W[i]=beta2*v_W[i]+(1-beta2)*layer.d_W**2\n",
    "                    v_b[i]=beta2*v_b[i]+(1-beta2)*layer.d_b**2\n",
    "                    \n",
    "                    m_W_hat=m_W[i]/(1-np.power(beta1,t+1))\n",
    "                    m_b_hat=m_b[i]/(1-np.power(beta1,t+1))\n",
    "                    v_W_hat=v_W[i]/(1-np.power(beta2,t+1))\n",
    "                    v_b_hat=v_b[i]/(1-np.power(beta2,t+1))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    layer.W=layer.W-(eta/(np.sqrt(v_W_hat)+epsilon))*\\\n",
    "                    (beta1*m_W_hat+((1-beta1)/(1-np.power(beta1,t+1)))*layer.d_W)\n",
    "                    layer.b=layer.b-(eta/(np.sqrt(v_b_hat)+epsilon))*\\\n",
    "                    (beta1*m_b_hat+((1-beta1)/(1-np.power(beta1,t+1)))*layer.d_b)\n",
    "                    \n",
    "                self.reset() #reset grads before new epoch\n",
    "            \n",
    "\n",
    "                \n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8a13369",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=optimizers(784,10,\n",
    "           [32,32,32],['sigmoid','sigmoid','sigmoid'],\n",
    "           loss='cross-entropy',optimizer='adam',\n",
    "           lamdba=0,batch_size=5,epochs=2,eta=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3b3b150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.14s/it]\n"
     ]
    }
   ],
   "source": [
    "opt.batch_gradient_descent((Xtrain,ytrain),(Xval,yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8448aac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19952.756417046214"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1197bf33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179535.2011654099"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1415a4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179535.2011654099"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=get_loss('cross-entropy')\n",
    "l(ytrain,opt.model.predict(Xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a02dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=opt.model.predict(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac8b6b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10403876, 0.09544095, 0.10933176, 0.10664418, 0.09939404,\n",
       "       0.09000446, 0.08961561, 0.10293147, 0.10225446, 0.10034431])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9fef2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d5d4a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10403876],\n",
       "       [0.09544095],\n",
       "       [0.10933176],\n",
       "       [0.10664418],\n",
       "       [0.09939404],\n",
       "       [0.09000446],\n",
       "       [0.08961561],\n",
       "       [0.10293147],\n",
       "       [0.10225446],\n",
       "       [0.10034431]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.model.forward(Xtrain[:,[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd691a55-115a-45fb-8215-aa5667619b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_NN=optimizers(784,10,[32,32,32],['sigmoid','sigmoid','sigmoid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9eee0d7a-2dc2-449a-8e96-627bd2428f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test_NN.stochastic_gradient_descent(X,Y)\n",
    "# # test_NN.Momentum_based(X,Y)\n",
    "# # test_NN.batch_gradient_descent(X,Y,batch_size=3,max_iters=10000)\n",
    "# # test_NN.rmsprop(X,Y)\n",
    "# # test_NN.adam(X,Y,eta=0.001,batch_size=2,max_iters=10000)\n",
    "# test_NN.stochastic_gradient_descent(Xtrain,ytrain,eta=0.001,max_iters=10)\n",
    "# # test_NN.adam(Xtrain,ytrain,eta=0.001,batch_size=64,max_iters=10)\n",
    "# # test_NN.NAdam(Xtrain,ytrain,eta=0.001,batch_size=64,max_iters=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "833e5a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ypred0=test_NN.forward(Xtrain[:,[0]])\n",
    "# ypred1=test_NN.forward(Xtrain[:,[1]])\n",
    "# ypred2=test_NN.forward(Xtrain[:,[2]])\n",
    "# ypred=np.hstack((ypred0,ypred1,ypred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36c6020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y=np.hstack((ytrain[:,[0]],ytrain[:,[1]],ytrain[:,[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0416f0ef-fdfa-4848-be1f-32d72aebbe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep small laerning rate, batch size<shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90fef019",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.transpose([[1,2,-3,4],[4,-5,6,7],[2,3,4,-5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e26b0516",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.transpose([[0,0,1,0],[0,1,0,0],[0,0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23d2baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60616e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/100000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'optimizers' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m test_NN\u001b[38;5;241m=\u001b[39moptimizers(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m4\u001b[39m,[\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m3\u001b[39m],[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m],loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcross-entropy\u001b[39m\u001b[38;5;124m'\u001b[39m,optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m            lamdba\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,eta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtest_NN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstochastic_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m ypred0\u001b[38;5;241m=\u001b[39mtest_NN\u001b[38;5;241m.\u001b[39mforward(a[:,[\u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m      5\u001b[0m ypred1\u001b[38;5;241m=\u001b[39mtest_NN\u001b[38;5;241m.\u001b[39mforward(a[:,[\u001b[38;5;241m1\u001b[39m]])\n",
      "Cell \u001b[0;32mIn [27], line 60\u001b[0m, in \u001b[0;36moptimizers.stochastic_gradient_descent\u001b[0;34m(self, X, Y, eta, max_iters)\u001b[0m\n\u001b[1;32m     58\u001b[0m x\u001b[38;5;241m=\u001b[39mX[:,[i]]\n\u001b[1;32m     59\u001b[0m y\u001b[38;5;241m=\u001b[39mY[:,[i]]\n\u001b[0;32m---> 60\u001b[0m y_pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward(x,y,y_pred)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#update\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [23], line 25\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m output\u001b[38;5;241m=\u001b[39mx\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(output.shape)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m  \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# print('W',layer.W.shape)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     output\u001b[38;5;241m=\u001b[39mlayer\u001b[38;5;241m.\u001b[39mforward(output)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# print(output.shape)   \u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'optimizers' object has no attribute 'layers'"
     ]
    }
   ],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'],loss='cross-entropy',optimizer='adam',\n",
    "           lamdba=0,batch_size=5,epochs=2,eta=1e-3)\n",
    "test_NN.stochastic_gradient_descent(a,b,eta=0.001,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a3108c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [00:02<00:00, 3650.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE:- 4.024540849343003\n",
      "[[0.01227045 0.01227045 0.01227037]\n",
      " [0.32925966 0.32925966 0.3292597 ]\n",
      " [0.3292268  0.3292268  0.32922681]\n",
      " [0.3292431  0.32924309 0.32924312]] \n",
      " [[0 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'],loss='cross-entropy',optimizer='adam',\n",
    "           lamdba=0,batch_size=5,epochs=10000,eta=1e-3)\n",
    "test_NN.batch_gradient_descent((a,b),(a,b))\n",
    "ypred0=test_NN.model.forward(a[:,[0]])\n",
    "ypred1=test_NN.model.forward(a[:,[1]])\n",
    "ypred2=test_NN.model.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d166564",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=optimizers_legacy(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.batch_gradient_descent(a,b,eta=1e-3,batch_size=5,max_iters=1000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e757a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.Momentum_based(a,b,eta=0.001,batch_size=2,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e15f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.rmsprop(a,b,eta=0.001,batch_size=2,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b027f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.adam(a,b,eta=0.001,batch_size=2,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3197faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.NAG(a,b,eta=0.001,batch_size=2,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad19aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.NAdam(a,b,eta=0.001,batch_size=2,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87dca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
