{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caab2aca-d1a8-4839-a5eb-db43e5e6542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a7ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "238d8eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 19:08:33.387279: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-13 19:08:33.460427: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-13 19:08:33.477675: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-13 19:08:33.731065: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-13 19:08:33.731096: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-13 19:08:33.731111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d789c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc49ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90955b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab7d1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)#sets a seed, used for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11b33e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(inarray): #converts to one hot encoding\n",
    "    outarray = np.zeros((inarray.size, inarray.max() + 1))\n",
    "    outarray[np.arange(inarray.size), inarray] = 1\n",
    "    return outarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4892ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess(X,y):\n",
    "      \n",
    "    '''Unrolls X,y, rehsapes into column vectors, one hots y'''\n",
    "    assert(X.shape[0]==y.shape[0]),\"Inputs must contain same number of examples, stored in rows\" #checks if same dim\n",
    "    X_processed=[]\n",
    "    y_processed=[]\n",
    "    \n",
    "    for i in range(np.shape(X)[0]):\n",
    "        X_processed.append(X_train[i].ravel())\n",
    "    y_processed=one_hot(y_train).T\n",
    "    return np.array(X_processed).T,y_processed\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00f95823",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean,y_train_clean=Preprocess(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "678e5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest,ytest=Preprocess(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0f18031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tran_val_split(X,y,split=0.1):\n",
    "    assert(X.shape[1]==y.shape[1]), \"Inputs must contain same number of examples, stored in columns\"# as vectors are now stored in cols, do check if no of elemnts are equal\n",
    "    len_split=int(np.shape(X)[1]*split)\n",
    "    np.random.shuffle(X)\n",
    "    np.random.shuffle(y)\n",
    "    X_val=X[:,:len_split]\n",
    "    y_val=y[:,:len_split]\n",
    "    \n",
    "    X_train=X[:,len_split:]\n",
    "    y_train=y[:,len_split:]\n",
    "    \n",
    "    return (X_train,y_train),(X_val,y_val)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9474fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Xtrain,ytrain),(Xval,yval)=tran_val_split(X_train_clean,y_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0464a583-6e66-4d4b-a9fe-d5c9529039a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    return np.where(x >= 0, \n",
    "                    1 / (1 + np.exp(-x)), \n",
    "                    np.exp(x) / (1 + np.exp(x)))\n",
    "def softmax(x):\n",
    "\n",
    "    z=x-np.max(x,axis=0) #doing this for numerical stability, prevents over/undeflow\n",
    "    return np.exp(z)/np.sum(np.exp(z),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1760ea39-c325-4e77-90a4-2bfd4fb3c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(activation):#maybe getderivatives here iteself ?\n",
    "    def sigmoid(x):\n",
    "        return np.where(x >= 0, \n",
    "                        1 / (1 + np.exp(-x)), \n",
    "                        np.exp(x) / (1 + np.exp(x)))\n",
    "    def softmax(x):\n",
    "        z=x-np.max(x,axis=0)\n",
    "        return np.exp(z)/np.sum(np.exp(z),axis=0)\n",
    "    if activation=='sigmoid':\n",
    "        return sigmoid\n",
    "    elif activation=='softmax':\n",
    "        return softmax\n",
    "    elif activation== 'tanh':\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "12f6598e-effc-4cda-b1b6-7aa640aad858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_derivative(activation):#maybe getderivatives here iteself ?\n",
    "    def sigmoid_d(x):\n",
    "        sig= np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "        return sig*(1-sig)\n",
    "    def softmax_d(x):\n",
    "        z=x-np.max(x,axis=0)\n",
    "        soft=np.exp(z)/np.sum(np.exp(z),axis=0)\n",
    "        return soft*(1-soft)\n",
    "    if activation=='sigmoid':\n",
    "        return sigmoid_d\n",
    "    elif activation=='softmax':\n",
    "        '''\n",
    "        need to think more, not required for backprop as we look directly at dL/da_l\n",
    "        But still, for the sake of completeness, and if user wants softmax in the middle?\n",
    "        d S(x_i) /d x_j= S(x_i)*(kronecker delta_i,j -S(x_j))\n",
    "        But we care about only dh_k,j/da_k,j So no need to implement d S(x_i) /d x_j\n",
    "        d S(x_i) /d x_i should suffice\n",
    "        so we get array of [ d S(x_1) /d x_1, d S(x_2) /d x_2, ....]\n",
    "        \n",
    "        For MSE loss after softmax, we need cross terms...\n",
    "        '''\n",
    "        \n",
    "        return softmax_d\n",
    "    elif activation=='tanh':\n",
    "        return 1-tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a0d0152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(loss):\n",
    "    \n",
    "   \n",
    "    \n",
    "        \n",
    "    def crossentropy(P,Q):\n",
    "        assert(P.shape==Q.shape), \"Inputs must be of same shape\"\n",
    "\n",
    "        return np.sum([-np.dot(P[:,i],np.log2(Q[:,i])) for i in range(P.shape[1])])\n",
    "    def SE(P,Q):\n",
    "        assert(P.shape==Q.shape), \"Inputs must be of same shape\"\n",
    "\n",
    "        return np.square(P-Q)\n",
    "    \n",
    "    if loss==\"cross-entropy\":\n",
    "        return crossentropy\n",
    "    return SE\n",
    "    \n",
    "    \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5247c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_derivative(loss):\n",
    "    def SE_d(y_in,y_pred_in):\n",
    "        '''\n",
    "        derivative of MSE after softmax is used to get probabs from a_L:\n",
    "        We need indicator because the all terms of y_true are required unlike cross-entropy where only y_pred[l] is required\n",
    "        Thus transforming the stacked indicator to y_true, not here...\n",
    "        \n",
    "        '''\n",
    "\n",
    "        def indicator(i,j):\n",
    "                if i==j:\n",
    "                    return 1\n",
    "                return 0\n",
    "\n",
    "\n",
    "        assert(y_in.shape[0]==y_pred_in.shape[0]),\"Inputs must contain same number of examples\"\n",
    "\n",
    "        y=y_in.ravel()\n",
    "        y_pred=y_pred_in.ravel()\n",
    "\n",
    "\n",
    "        return np.array([\n",
    "            [2*np.sum([(y_pred[i]-y[i])*y[i]*(indicator(i,j) - y_pred[j]) for i in range(y.shape[0])])]\n",
    "            for j in range(len(y))\n",
    "        ])    \n",
    "   \n",
    "    \n",
    "        \n",
    "    def crossentropy_d(y,y_pred):\n",
    "        \n",
    "\n",
    "        return -(y-y_pred)\n",
    "    \n",
    "    \n",
    "    if loss==\"cross-entropy\":\n",
    "        return crossentropy_d\n",
    "    return SE_d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bacbe744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def SE_d(y_in,y_pred_in):\n",
    "    \n",
    "#     def indicator(i,j):\n",
    "#             if i==j:\n",
    "#                 return 1\n",
    "#             return 0\n",
    "        \n",
    "        \n",
    "#     assert(y_in.shape[0]==y_pred_in.shape[0]),\"Inputs must contain same number of examples\"\n",
    "\n",
    "#     y=y_in.ravel()\n",
    "#     y_pred=y_pred_in.ravel()\n",
    "\n",
    "\n",
    "#     return np.array([\n",
    "#         [2*np.sum([(y_pred[i]-y[i])*y[i]*(indicator(i,j) - y_pred[j]) for i in range(y.shape[0])])]\n",
    "#         for j in range(len(y))\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83131eae-4c3a-4139-b5c6-65bc10c558c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self,input_size,output_size,activation='sigmoid'):\n",
    "            \n",
    "        ''' \n",
    "        output size number of neurons i\n",
    "        input size j\n",
    "        \n",
    "        '''\n",
    "        self.W=np.random.randn(output_size,input_size) #size ixj\n",
    "        self.b=np.random.randn(output_size,1)           #size i\n",
    "        self.a=np.random.randn(output_size,1)           #size i\n",
    "        self.h=np.random.randn(output_size,1)           #size i\n",
    "        self.g=get_activation(activation)\n",
    "        \n",
    "        self.d_a=np.zeros((output_size,1))\n",
    "        self.d_h=np.zeros((output_size,1))\n",
    "        self.d_W=np.zeros((output_size,input_size))\n",
    "        self.d_b=np.zeros((output_size,1))\n",
    "        self.d_g=get_activation_derivative(activation)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.a=self.b+np.matmul(self.W,inputs)\n",
    "        self.h=self.g(self.a)\n",
    "        return self.h\n",
    "    def reset(self):\n",
    "        self.d_a=np.zeros(np.shape(self.d_a))\n",
    "        self.d_h=np.zeros(np.shape(self.d_h))\n",
    "        self.d_W=np.zeros(np.shape(self.d_W))\n",
    "        self.d_b=np.zeros(np.shape(self.d_b))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8cd88683-59f7-48b6-8f02-a61382ecc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,X_size,Y_size,hidden_layer_sizes=[4],hidden_layer_activations=['sigmoid'],loss='cross-entropy'):\n",
    "        '''\n",
    "        '''   \n",
    "        \n",
    "        self.input_size=X_size\n",
    "        self.output_size=Y_size\n",
    "        self.hidden_layer_sizes=hidden_layer_sizes\n",
    "        self.layers=[]\n",
    "        \n",
    "        prev_size=self.input_size\n",
    "    \n",
    "        for size,activation in zip(hidden_layer_sizes,hidden_layer_activations):\n",
    "            self.layers.append(layer(prev_size,size,activation))\n",
    "            prev_size=size\n",
    "        self.layers.append(layer(size,self.output_size,'softmax'))\n",
    "        \n",
    "        self.loss=get_loss(loss)\n",
    "        self.loss_d=get_loss_derivative(loss)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output=x\n",
    "        # print(output.shape)\n",
    "        for layer in  self.layers:\n",
    "            # print('W',layer.W.shape)\n",
    "            output=layer.forward(output)\n",
    "            # print(output.shape)   \n",
    "        return output\n",
    "    \n",
    "    def reset(self):\n",
    "        for layer in  self.layers:\n",
    "            # resets the dWs\n",
    "            layer.reset()\n",
    "        \n",
    "    \n",
    "          \n",
    "            \n",
    "    def backward(self,x,y,y_pred):\n",
    "        # self.layers[-1].d_h is not needed as d_h is used to calculate d_a and self.layers[-1].h is softmax\n",
    "        self.layers[-1].d_a=self.loss_d\n",
    "            \n",
    "        \n",
    "        \n",
    "        for idx in range(len(self.layers)-1,0,-1): #goes from L->2, for l=1 we do outside\n",
    "            \n",
    "            \n",
    "            #compute gradient wrt parameters\n",
    "            self.layers[idx].d_W+=np.matmul(self.layers[idx].d_a,np.transpose(self.layers[idx-1].h))\n",
    "            self.layers[idx].d_b+=self.layers[idx].d_a\n",
    "            \n",
    "            #compute gradient wrt layer below -- will help in next layer iter\n",
    "            self.layers[idx-1].d_h=np.matmul(np.transpose(self.layers[idx].W),self.layers[idx].d_a)\n",
    "            \n",
    "            #compute gradient -- element wise multiplivation, derivative of the activation function of layer idx-1\n",
    "            self.layers[idx-1].d_a=self.layers[idx-1].d_h*self.layers[idx-1].d_g(self.layers[idx-1].a)\n",
    "                        \n",
    "        self.layers[0].d_W+=np.matmul(self.layers[0].d_a,np.transpose(x))\n",
    "        self.layers[0].d_b+=self.layers[0].d_a \n",
    "        \n",
    "    def predict(self,Xtest):\n",
    "        preds=[]\n",
    "        for i in range(Xtest.shape[1]):\n",
    "            preds.append(self.forward(Xtest[:,[i]]))\n",
    "        \n",
    "        ytest_pred=np.hstack(preds)\n",
    "        return ytest_pred\n",
    "        \n",
    "        \n",
    "                    \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "29105d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_model=Model(784,10,[32],['sigmoid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7b353538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1921164/3051356264.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  1 / (1 + np.exp(-x)),\n",
      "/tmp/ipykernel_1921164/3051356264.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(x) / (1 + np.exp(x)))\n",
      "/tmp/ipykernel_1921164/3051356264.py:5: RuntimeWarning: invalid value encountered in divide\n",
      "  np.exp(x) / (1 + np.exp(x)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 10000)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_model.predict(Xtest).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019f0db-4847-4420-80c0-a93ab6c65254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizers(Model):\n",
    "    def __init__(self,X_size,Y_size,hidden_layer_sizes=[4],hidden_layer_activations=['sigmoid'],loss='cross-entropy'):\n",
    "        super().__init__(X_size,Y_size,hidden_layer_sizes,hidden_layer_activations,loss)\n",
    "\n",
    "    def batch_gradient_descent(self,X,Y,eta=1e-3,batch_size=1,max_iters=1000):\n",
    "        \n",
    "        '''\n",
    "        Mini-Batch Gradient Descent\n",
    "        at batchsize=1, behaves like sgd, batchsize=np.shape(X), behaves as gd\n",
    "        eta is the learning rate\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "                #update if the number of points seen==batch size, or if data ends\n",
    "                if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                    for layer in self.layers:\n",
    "                        layer.W=layer.W-eta*layer.d_W\n",
    "                        layer.b=layer.b-eta*layer.d_b\n",
    "                    self.reset() #reset grads before new epoch\n",
    "            \n",
    "            \n",
    "    def stochastic_gradient_descent(self,X,Y,eta=1e-3,max_iters=10):\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        stochastic Gradient Descent\n",
    "        '''\n",
    "                \n",
    "        \n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                \n",
    "                #update\n",
    "                for layer in self.layers:\n",
    "                    layer.W=layer.W-eta*layer.d_W\n",
    "                    layer.b=layer.b-eta*layer.d_b\n",
    "                self.reset() #reset grads before new update\n",
    "                    \n",
    "            \n",
    "        \n",
    "        \n",
    "    def Momentum(self,X,Y,eta=1e-3,beta=0.9,batch_size=100,max_iters=10):\n",
    "        ''''''\n",
    "        u_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        u_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    u_W[i]=beta*u_W[i]+layer.d_W\n",
    "                    u_b[i]=beta*u_b[i]+layer.d_b\n",
    "                    layer.W=layer.W-eta*u_W[i]\n",
    "                    layer.b=layer.b-eta*u_b[i]\n",
    "                self.reset() #reset grads before new update\n",
    "                \n",
    "\n",
    "\n",
    "    def rmsprop(self,X,Y,eta=1e-3,beta=0.9,batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    v_W[i]=beta*v_W[i]+(1-beta)*layer.d_W**2\n",
    "                    v_b[i]=beta*v_b[i]+(1-beta)*layer.d_b**2\n",
    "                    layer.W=layer.W-(eta/np.sqrt(v_W[i]+epsilon))*layer.d_W\n",
    "                    layer.b=layer.b-(eta/np.sqrt(v_b[i]+epsilon))*layer.d_b\n",
    "                self.reset() #reset grads before new epoch\n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "    def Adam(self,X,Y,eta=1e-3,beta1=0.9, beta2=0.999, batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    #updating momentum, velocity\n",
    "                    m_W[i]=beta1*m_W[i]+(1-beta1)*layer.d_W\n",
    "                    m_b[i]=beta1*m_b[i]+(1-beta1)*layer.d_b\n",
    "                    \n",
    "                    v_W[i]=beta2*v_W[i]+(1-beta2)*layer.d_W**2\n",
    "                    v_b[i]=beta2*v_b[i]+(1-beta2)*layer.d_b**2\n",
    "                    \n",
    "                    m_W_hat=m_W[i]/(1-np.power(beta1,t+1))\n",
    "                    m_b_hat=m_b[i]/(1-np.power(beta1,t+1))\n",
    "                    v_W_hat=v_W[i]/(1-np.power(beta2,t+1))\n",
    "                    v_b_hat=v_b[i]/(1-np.power(beta2,t+1))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    layer.W=layer.W-(eta*m_W_hat)/(np.sqrt(v_W_hat)+epsilon)\n",
    "                    layer.b=layer.b-(eta*m_b_hat)/(np.sqrt(v_b_hat)+epsilon)\n",
    "                self.reset() #reset grads before new epoch\n",
    "    \n",
    "    def NAG(self,X,Y,eta=1e-3,beta=0.9,batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    m_W[i]=beta*m_W[i]+eta*layer.d_W\n",
    "                    m_b[i]=beta*m_b[i]+eta*layer.d_b\n",
    "\n",
    "                    \n",
    "                    layer.W=layer.W-(beta*m_W[i]+eta*layer.d_W[i])\n",
    "                    layer.b=layer.b-(beta*m_b[i]+eta*layer.d_b[i])\n",
    "                self.reset() #reset grads before new epoch\n",
    "                \n",
    "                \n",
    "    \n",
    "    def NAdam(self,X,Y,eta=1e-3,beta1=0.9, beta2=0.999, batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    #updating momentum, velocity\n",
    "                    m_W[i]=beta1*m_W[i]+(1-beta1)*layer.d_W\n",
    "                    m_b[i]=beta1*m_b[i]+(1-beta1)*layer.d_b\n",
    "                    \n",
    "                    v_W[i]=beta2*v_W[i]+(1-beta2)*layer.d_W**2\n",
    "                    v_b[i]=beta2*v_b[i]+(1-beta2)*layer.d_b**2\n",
    "                    \n",
    "                    m_W_hat=m_W[i]/(1-np.power(beta1,t+1))\n",
    "                    m_b_hat=m_b[i]/(1-np.power(beta1,t+1))\n",
    "                    v_W_hat=v_W[i]/(1-np.power(beta2,t+1))\n",
    "                    v_b_hat=v_b[i]/(1-np.power(beta2,t+1))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    layer.W=layer.W-(eta/(np.sqrt(v_W_hat)+epsilon))*\\\n",
    "                    (beta1*m_W_hat+((1-beta1)/(1-np.power(beta1,t+1)))*layer.d_W)\n",
    "                    layer.b=layer.b-(eta/(np.sqrt(v_b_hat)+epsilon))*\\\n",
    "                    (beta1*m_b_hat+((1-beta1)/(1-np.power(beta1,t+1)))*layer.d_b)\n",
    "                    \n",
    "                self.reset() #reset grads before new epoch\n",
    "            \n",
    "\n",
    "                \n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd691a55-115a-45fb-8215-aa5667619b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_NN=optimizers(784,10,[32,32,32],['sigmoid','sigmoid','sigmoid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee0d7a-2dc2-449a-8e96-627bd2428f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test_NN.stochastic_gradient_descent(X,Y)\n",
    "# # test_NN.Momentum_based(X,Y)\n",
    "# # test_NN.batch_gradient_descent(X,Y,batch_size=3,max_iters=10000)\n",
    "# # test_NN.rmsprop(X,Y)\n",
    "# # test_NN.adam(X,Y,eta=0.001,batch_size=2,max_iters=10000)\n",
    "# test_NN.stochastic_gradient_descent(Xtrain,ytrain,eta=0.001,max_iters=10)\n",
    "# # test_NN.adam(Xtrain,ytrain,eta=0.001,batch_size=64,max_iters=10)\n",
    "# # test_NN.NAdam(Xtrain,ytrain,eta=0.001,batch_size=64,max_iters=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e5a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ypred0=test_NN.forward(Xtrain[:,[0]])\n",
    "# ypred1=test_NN.forward(Xtrain[:,[1]])\n",
    "# ypred2=test_NN.forward(Xtrain[:,[2]])\n",
    "# ypred=np.hstack((ypred0,ypred1,ypred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y=np.hstack((ytrain[:,[0]],ytrain[:,[1]],ytrain[:,[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416f0ef-fdfa-4848-be1f-32d72aebbe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep small laerning rate, batch size<shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fef019",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.transpose([[1,2,-3,4],[4,-5,6,7],[2,3,4,-5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26b0516",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.transpose([[0,0,1,0],[0,1,0,0],[0,0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60616e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.stochastic_gradient_descent(a,b,eta=0.001,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d166564",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.batch_gradient_descent(a,b,eta=0.001,batch_size=2,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e757a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.Momentum_based(a,b,eta=0.001,batch_size=2,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e15f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.rmsprop(a,b,eta=0.001,batch_size=2,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b027f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.adam(a,b,eta=0.001,batch_size=2,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3197faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.NAG(a,b,eta=0.001,batch_size=2,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad19aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.NAdam(a,b,eta=0.001,batch_size=2,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87dca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
