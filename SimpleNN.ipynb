{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efd9ae12-49e0-43a6-9016-a90bb33fa54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.datasets import fashion_mnist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caab2aca-d1a8-4839-a5eb-db43e5e6542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc99d022-59a3-4b53-841c-818670c21225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_train, y_train), (x_test, y_test)=fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0464a583-6e66-4d4b-a9fe-d5c9529039a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def softmax(x):\n",
    "\n",
    "    z=x-np.max(x,axis=0) #doing this for numerical stability, prevents over/undeflow\n",
    "    return np.exp(z)/np.sum(np.exp(z),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1760ea39-c325-4e77-90a4-2bfd4fb3c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(activation):#maybe getderivatives here iteself ?\n",
    "    def sigmoid(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    def softmax(x):\n",
    "        z=x-np.max(x,axis=0)\n",
    "        return np.exp(z)/np.sum(np.exp(z),axis=0)\n",
    "    if activation=='sigmoid':\n",
    "        return sigmoid\n",
    "    elif activation=='softmax':\n",
    "        return softmax\n",
    "    elif activation== 'tanh':\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12f6598e-effc-4cda-b1b6-7aa640aad858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_derivative(activation):#maybe getderivatives here iteself ?\n",
    "    def sigmoid_d(x):\n",
    "        sig=1/(1+np.exp(-x))\n",
    "        return sig*(1-sig)\n",
    "    def softmax_d(x):\n",
    "        soft=np.exp(x)/np.sum(np.exp(x),axis=0)\n",
    "        return soft*(1-soft)\n",
    "    if activation=='sigmoid':\n",
    "        return sigmoid_d\n",
    "    elif activation=='softmax':\n",
    "        '''\n",
    "        need to think more, not required for backprop as we look directly at dL/da_l\n",
    "        But still, for the sake of completeness, and if user wants softmax in the middle?\n",
    "        d S(x_i) /d x_j= S(x_i)*(kronecker delta_i,j -S(x_j))\n",
    "        But we care about only dh_k,j/da_k,j So no need to implement d S(x_i) /d x_j\n",
    "        d S(x_i) /d x_i should suffice\n",
    "        so we get array of [ d S(x_1) /d x_1, d S(x_2) /d x_2, ....]\n",
    "        '''\n",
    "        \n",
    "        return softmax_d\n",
    "    elif activation=='tanh':\n",
    "        return 1-tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83131eae-4c3a-4139-b5c6-65bc10c558c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self,input_size,output_size,activation='sigmoid'):\n",
    "            \n",
    "        ''' \n",
    "        output size number of neurons i\n",
    "        input size j\n",
    "        \n",
    "        '''\n",
    "        self.W=np.random.randn(output_size,input_size) #size ixj\n",
    "        self.b=np.random.randn(output_size,1)           #size i\n",
    "        self.a=np.random.randn(output_size,1)           #size i\n",
    "        self.h=np.random.randn(output_size,1)           #size i\n",
    "        self.g=get_activation(activation)\n",
    "        \n",
    "        self.d_a=np.random.randn(output_size,1)\n",
    "        self.d_h=np.random.randn(output_size,1)\n",
    "        self.d_W=np.random.randn(output_size,input_size)\n",
    "        self.d_b=np.random.randn(output_size,1)\n",
    "        self.d_g=get_activation_derivative(activation)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.a=self.b+np.matmul(self.W,inputs)\n",
    "        self.h=self.g(self.a)\n",
    "        return self.h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3d1d21d-2990-4af0-9d23-797623e23bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=[1,2,3,4]\n",
    "temp1=[1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d96b29f8-8e4f-425a-b0c9-e5c1588f1a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "2 2\n",
      "3 3\n",
      "4 4\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(temp,temp1):\n",
    "    \n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9c04d9e-da2e-4f50-9567-794f2ee0ee5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3\n",
      "3 2\n",
      "2 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(temp)-1,0,-1):\n",
    "    \n",
    "    print (temp[i],temp[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8cd88683-59f7-48b6-8f02-a61382ecc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,X_size,Y_size,hidden_layer_sizes=[4],hidden_layer_activations=['sigmoid']):\n",
    "        '''\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.input_size=X_size\n",
    "        self.output_size=Y_size\n",
    "        self.hidden_layer_sizes=hidden_layer_sizes\n",
    "        self.layers=[]\n",
    "        \n",
    "        prev_size=self.input_size\n",
    "    \n",
    "        for size,activation in zip(hidden_layer_sizes,hidden_layer_activations):\n",
    "            self.layers.append(layer(prev_size,size,activation))\n",
    "            prev_size=size\n",
    "        self.layers.append(layer(size,self.output_size,'softmax'))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output=x\n",
    "        # print(output.shape)\n",
    "        for layer in  self.layers:\n",
    "            # print('W',layer.W.shape)\n",
    "            output=layer.forward(output)\n",
    "            # print(output.shape)   \n",
    "        return output\n",
    "    \n",
    "    def backward(self,x,y,y_pred):\n",
    "        # self.layers[-1].d_h is not needed as d_h is used to calculate d_a and self.layers[-1].h is softmax\n",
    "        self.layers[-1].d_a=-(y-y_pred)\n",
    "        \n",
    "        \n",
    "        for idx in range(len(self.layers)-1,0,-1): #goes from L->2, for l=1 we do outside\n",
    "            \n",
    "            \n",
    "            #compute gradient wrt parameters\n",
    "            self.layers[idx].d_W=np.matmul(self.layers[idx].d_a,np.transpose(self.layers[idx-1].h))\n",
    "            self.layers[idx].d_b=self.layers[idx].d_a\n",
    "            \n",
    "            #compute gradient wrt layer below -- will help in next layer iter\n",
    "            self.layers[idx-1].d_h=np.matmul(np.transpose(self.layers[idx].W),self.layers[idx].d_a)\n",
    "            \n",
    "            #compute gradient -- element wise multiplivation, derivative of the activation function of layer idx-1\n",
    "            self.layers[idx-1].d_a=self.layers[idx-1].d_h*self.layers[idx-1].d_g(self.layers[idx-1].a)\n",
    "                        \n",
    "        self.layers[0].d_W=np.matmul(self.layers[0].d_a,np.transpose(x))\n",
    "        self.layers[0].d_b=self.layers[0].d_a \n",
    "        \n",
    "    def batch_gradient_descent(self,X,Y,eta=1,batch_size=1):\n",
    "        \n",
    "        '''\n",
    "        Mini-Batch Gradient Descent\n",
    "        at batchsize=1, behaves like sgd, batchsize=np.shape(X), behaves as gd\n",
    "        '''\n",
    "        t=1\n",
    "        max_iters=1000\n",
    "        \n",
    "        \n",
    "\n",
    "        while t<max_iters:\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                \n",
    "                #update if the number of points seen==batch size, or if \n",
    "                if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[0]:\n",
    "                    for layer in self.layers:\n",
    "                        layer.W=layer.W-eta*layer.d_W\n",
    "                        layer.b=layer.b-eta*layer.d_b\n",
    "            t+=1\n",
    "            \n",
    "    def stochastic_gradient_descent(self,X,Y,eta=1,batch_size=1):\n",
    "        \n",
    "        '''\n",
    "        stochastic Gradient Descent\n",
    "        '''\n",
    "        t=1\n",
    "        max_iters=1000\n",
    "        \n",
    "        \n",
    "\n",
    "        while t<max_iters:\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                \n",
    "                #update\n",
    "                for layer in self.layers:\n",
    "                    layer.W=layer.W-eta*layer.d_W\n",
    "                    layer.b=layer.b-eta*layer.d_b\n",
    "            t+=1\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "51b21406-0c40-4acd-b9f6-27448a5291c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 25, 30, 35])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,1]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e67194dc-0d08-46e9-976a-6bcff09fc176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3]]),\n",
       " array([[4],\n",
       "        [4],\n",
       "        [4],\n",
       "        [4],\n",
       "        [4]])]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_split([X[:,0]]*5,2,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "647221b2-a90d-4099-a3ff-d4f024509a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 7])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(X,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8a600e5-d77d-460f-bc04-28816d99e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=[1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d569f2c-eba0-448f-ba70-e13f8ce93558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(temp)-1,0,-1):\n",
    "    # print (temp[idx],temp[idx-1])    \n",
    "    print (temp[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "bd691a55-115a-45fb-8215-aa5667619b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=NeuralNetwork(4,5,[3,1],['sigmoid','sigmoid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "84d726ed-50b8-447b-a618-30c99d03ad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.transpose([[1,2,3,4],[4,5,6,7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "830cfcca-31d4-4700-967f-b2d47dcfcc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=np.transpose([[0,0,1,1,1],[1,1,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ce7f6dcf-7d07-4c6a-bb09-02d004047536",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN.stochastic_gradient_descent(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "682b4173-7fcf-475c-9957-4a207f91cc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.39801483],\n",
       "       [11.45417734],\n",
       "       [16.58635686]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_NN.layers[0].a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "4bf4c688-e133-4359-84d7-53e4fe16fa6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1197.27883803],\n",
       "       [1197.27883801],\n",
       "       [1198.65485279],\n",
       "       [1198.65485278],\n",
       "       [1198.65485278]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_NN.layers[-1].a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "3f7316de-0d31-412b-a5cc-c033c6262c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praga\\AppData\\Local\\Temp\\ipykernel_16984\\3960807903.py:1: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(test_NN.layers[-1].a)/np.sum(np.exp(test_NN.layers[-1].a))\n",
      "C:\\Users\\praga\\AppData\\Local\\Temp\\ipykernel_16984\\3960807903.py:1: RuntimeWarning: invalid value encountered in divide\n",
      "  np.exp(test_NN.layers[-1].a)/np.sum(np.exp(test_NN.layers[-1].a))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(test_NN.layers[-1].a)/np.sum(np.exp(test_NN.layers[-1].a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "84f4f4c2-4b4c-4565-a069-4915efb7efa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07206025],\n",
       "       [0.07206025],\n",
       "       [0.28529317],\n",
       "       [0.28529317],\n",
       "       [0.28529317]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_NN.layers[-1].h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
