{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caab2aca-d1a8-4839-a5eb-db43e5e6542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44a7ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "238d8eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 11:55:00.344786: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 11:55:00.427720: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-14 11:55:00.446009: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-14 11:55:00.699194: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-14 11:55:00.699243: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-14 11:55:00.699246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d789c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fc49ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90955b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab7d1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11b33e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(a):\n",
    "    b = np.zeros((a.size, a.max() + 1))\n",
    "    b[np.arange(a.size), a] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82c08978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4892ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess(X,y):\n",
    "    '''Unrolls X,y, rehsapes into column vectors, one hots y'''\n",
    "    X_processed=[]\n",
    "    y_processed=[]\n",
    "    \n",
    "    for i in range(np.shape(X)[0]):\n",
    "        X_processed.append(X_train[i].ravel())\n",
    "    y_processed=one_hot(y_train.ravel()).T\n",
    "    return np.array(X_processed).T,y_processed\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00f95823",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean,y_train_clean=Preprocess(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "678e5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest,ytest=Preprocess(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0f18031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tran_val_split(X,y,split=0.1):\n",
    "    len_split=int(np.shape(X)[1]*split)\n",
    "    np.random.shuffle(X)\n",
    "    np.random.shuffle(y)\n",
    "    X_val=X[:,:len_split]\n",
    "    y_val=y[:,:len_split]\n",
    "    \n",
    "    X_train=X[:,len_split:]\n",
    "    y_train=y[:,len_split:]\n",
    "    \n",
    "    return (X_train,y_train),(X_val,y_val)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9474fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Xtrain,ytrain),(Xval,yval)=tran_val_split(X_train_clean,y_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5481a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_loss(loss='logloss'):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0464a583-6e66-4d4b-a9fe-d5c9529039a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    return np.where(x >= 0, \n",
    "                    1 / (1 + np.exp(-x)), \n",
    "                    np.exp(x) / (1 + np.exp(x)))\n",
    "def softmax(x):\n",
    "\n",
    "    z=x-np.max(x,axis=0) #doing this for numerical stability, prevents over/undeflow\n",
    "    return np.exp(z)/np.sum(np.exp(z),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1760ea39-c325-4e77-90a4-2bfd4fb3c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(activation):#maybe getderivatives here iteself ?\n",
    "    def sigmoid(x):\n",
    "        return np.where(x >= 0, \n",
    "                        1 / (1 + np.exp(-x)), \n",
    "                        np.exp(x) / (1 + np.exp(x)))\n",
    "    def softmax(x):\n",
    "        z=x-np.max(x,axis=0)\n",
    "        return np.exp(z)/np.sum(np.exp(z),axis=0)\n",
    "    if activation=='sigmoid':\n",
    "        return sigmoid\n",
    "    elif activation=='softmax':\n",
    "        return softmax\n",
    "    elif activation== 'tanh':\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12f6598e-effc-4cda-b1b6-7aa640aad858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_derivative(activation):#maybe getderivatives here iteself ?\n",
    "    def sigmoid_d(x):\n",
    "        sig= np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "        sig=1/(1+np.exp(-x))\n",
    "        return sig*(1-sig)\n",
    "    def softmax_d(x):\n",
    "        z=x-np.max(x,axis=0)\n",
    "        soft=np.exp(z)/np.sum(np.exp(z),axis=0)\n",
    "        return soft*(1-soft)\n",
    "    if activation=='sigmoid':\n",
    "        return sigmoid_d\n",
    "    elif activation=='softmax':\n",
    "        '''\n",
    "        need to think more, not required for backprop as we look directly at dL/da_l\n",
    "        But still, for the sake of completeness, and if user wants softmax in the middle?\n",
    "        d S(x_i) /d x_j= S(x_i)*(kronecker delta_i,j -S(x_j))\n",
    "        But we care about only dh_k,j/da_k,j So no need to implement d S(x_i) /d x_j\n",
    "        d S(x_i) /d x_i should suffice\n",
    "        so we get array of [ d S(x_1) /d x_1, d S(x_2) /d x_2, ....]\n",
    "        '''\n",
    "        \n",
    "        return softmax_d\n",
    "    elif activation=='tanh':\n",
    "        return 1-tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83131eae-4c3a-4139-b5c6-65bc10c558c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self,input_size,output_size,activation='sigmoid'):\n",
    "            \n",
    "        ''' \n",
    "        output size number of neurons i\n",
    "        input size j\n",
    "        \n",
    "        '''\n",
    "        self.W=np.random.randn(output_size,input_size)*0.01 #size ixj\n",
    "        self.b=np.random.randn(output_size,1)*0.01           #size i\n",
    "        self.a=np.random.randn(output_size,1)*0.01           #size i\n",
    "        self.h=np.random.randn(output_size,1)*0.01           #size i\n",
    "        self.g=get_activation(activation)\n",
    "        \n",
    "        self.d_a=np.zeros((output_size,1))\n",
    "        self.d_h=np.zeros((output_size,1))\n",
    "        self.d_W=np.zeros((output_size,input_size))\n",
    "        self.d_b=np.zeros((output_size,1))\n",
    "        self.d_g=get_activation_derivative(activation)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.a=self.b+np.matmul(self.W,inputs)\n",
    "        self.h=self.g(self.a)\n",
    "        return self.h\n",
    "    def reset(self):\n",
    "        self.d_a=np.zeros(np.shape(self.d_a))\n",
    "        self.d_h=np.zeros(np.shape(self.d_h))\n",
    "        self.d_W=np.zeros(np.shape(self.d_W))\n",
    "        self.d_b=np.zeros(np.shape(self.d_b))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3d1d21d-2990-4af0-9d23-797623e23bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=[1,2,3,4]\n",
    "temp1=[1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cd88683-59f7-48b6-8f02-a61382ecc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,X_size,Y_size,hidden_layer_sizes=[4],hidden_layer_activations=['sigmoid']):\n",
    "        '''\n",
    "        '''   \n",
    "        \n",
    "        self.input_size=X_size\n",
    "        self.output_size=Y_size\n",
    "        self.hidden_layer_sizes=hidden_layer_sizes\n",
    "        self.layers=[]\n",
    "        \n",
    "        prev_size=self.input_size\n",
    "    \n",
    "        for size,activation in zip(hidden_layer_sizes,hidden_layer_activations):\n",
    "            self.layers.append(layer(prev_size,size,activation))\n",
    "            prev_size=size\n",
    "        self.layers.append(layer(size,self.output_size,'softmax'))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output=x\n",
    "        # print(output.shape)\n",
    "        for layer in  self.layers:\n",
    "            # print('W',layer.W.shape)\n",
    "            output=layer.forward(output)\n",
    "            # print(output.shape)   \n",
    "        return output\n",
    "    \n",
    "    def reset(self):\n",
    "        for layer in  self.layers:\n",
    "            # resets the dWs\n",
    "            layer.reset()\n",
    "        \n",
    "    \n",
    "          \n",
    "            \n",
    "    def backward(self,x,y,y_pred):\n",
    "        # self.layers[-1].d_h is not needed as d_h is used to calculate d_a and self.layers[-1].h is softmax\n",
    "        self.layers[-1].d_a=-(y-y_pred)\n",
    "        \n",
    "        \n",
    "        for idx in range(len(self.layers)-1,0,-1): #goes from L->2, for l=1 we do outside\n",
    "            \n",
    "            \n",
    "            #compute gradient wrt parameters\n",
    "            self.layers[idx].d_W+=np.matmul(self.layers[idx].d_a,np.transpose(self.layers[idx-1].h))\n",
    "            self.layers[idx].d_b+=self.layers[idx].d_a\n",
    "            \n",
    "            #compute gradient wrt layer below -- will help in next layer iter\n",
    "            self.layers[idx-1].d_h=np.matmul(np.transpose(self.layers[idx].W),self.layers[idx].d_a)\n",
    "            \n",
    "            #compute gradient -- element wise multiplivation, derivative of the activation function of layer idx-1\n",
    "            self.layers[idx-1].d_a=self.layers[idx-1].d_h*self.layers[idx-1].d_g(self.layers[idx-1].a)\n",
    "                        \n",
    "        self.layers[0].d_W+=np.matmul(self.layers[0].d_a,np.transpose(x))\n",
    "        self.layers[0].d_b+=self.layers[0].d_a \n",
    "        \n",
    "        \n",
    "                    \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9019f0db-4847-4420-80c0-a93ab6c65254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizers(Model):\n",
    "    def __init__(self,X_size,Y_size,hidden_layer_sizes=[4],hidden_layer_activations=['sigmoid']):\n",
    "        super().__init__(X_size,Y_size,hidden_layer_sizes,hidden_layer_activations)\n",
    "\n",
    "    def batch_gradient_descent(self,X,Y,eta=1,batch_size=1,max_iters=1000):\n",
    "        \n",
    "        '''\n",
    "        Mini-Batch Gradient Descent\n",
    "        at batchsize=1, behaves like sgd, batchsize=np.shape(X), behaves as gd\n",
    "        eta is the learning rate\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "                #update if the number of points seen==batch size, or if data ends\n",
    "                if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                    for layer in self.layers:\n",
    "                        layer.W=layer.W-eta*layer.d_W\n",
    "                        layer.b=layer.b-eta*layer.d_b\n",
    "                    self.reset() #reset grads before new epoch\n",
    "            \n",
    "            \n",
    "    def stochastic_gradient_descent(self,X,Y,eta=1,max_iters=1000):\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        stochastic Gradient Descent\n",
    "        '''\n",
    "                \n",
    "        \n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                \n",
    "                #update\n",
    "                for layer in self.layers:\n",
    "                    layer.W=layer.W-eta*layer.d_W\n",
    "                    layer.b=layer.b-eta*layer.d_b\n",
    "                self.reset() #reset grads before new update\n",
    "                    \n",
    "            \n",
    "        \n",
    "        \n",
    "    def Momentum_based(self,X,Y,eta=1,beta=0.9,batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        u_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        u_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    u_W[i]=beta*u_W[i]+layer.d_W\n",
    "                    u_b[i]=beta*u_b[i]+layer.d_b\n",
    "                    layer.W=layer.W-eta*u_W[i]\n",
    "                    layer.b=layer.b-eta*u_b[i]\n",
    "                self.reset() #reset grads before new update\n",
    "                \n",
    "\n",
    "\n",
    "    def rmsprop(self,X,Y,eta=1,beta=0.9,batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    v_W[i]=beta*v_W[i]+(1-beta)*layer.d_W**2\n",
    "                    v_b[i]=beta*v_b[i]+(1-beta)*layer.d_b**2\n",
    "                    layer.W=layer.W-(eta/np.sqrt(v_W[i]+epsilon))*layer.d_W\n",
    "                    layer.b=layer.b-(eta/np.sqrt(v_b[i]+epsilon))*layer.d_b\n",
    "                self.reset() #reset grads before new epoch\n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "    def adam(self,X,Y,eta=1,beta1=0.9, beta2=0.999, batch_size=100,max_iters=1000):\n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    #updating momentum, velocity\n",
    "                    m_W[i]=beta1*m_W[i]+(1-beta1)*layer.d_W\n",
    "                    m_b[i]=beta1*m_b[i]+(1-beta1)*layer.d_b\n",
    "                    \n",
    "                    v_W[i]=beta2*v_W[i]+(1-beta2)*layer.d_W**2\n",
    "                    v_b[i]=beta2*v_b[i]+(1-beta2)*layer.d_b**2\n",
    "                    \n",
    "                    m_W_hat=m_W[i]/(1-np.power(beta1,t+1))\n",
    "                    m_b_hat=m_b[i]/(1-np.power(beta1,t+1))\n",
    "                    v_W_hat=v_W[i]/(1-np.power(beta2,t+1))\n",
    "                    v_b_hat=v_b[i]/(1-np.power(beta2,t+1))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    layer.W=layer.W-(eta*m_W_hat)/(np.sqrt(v_W_hat)+epsilon)\n",
    "                    layer.b=layer.b-(eta*m_b_hat)/(np.sqrt(v_b_hat)+epsilon)\n",
    "                self.reset() #reset grads before new epoch\n",
    "    \n",
    "    def NAG(self,X,Y,eta=1,beta=0.9,batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    m_W[i]=beta*m_W[i]+eta*layer.d_W\n",
    "                    m_b[i]=beta*m_b[i]+eta*layer.d_b\n",
    "\n",
    "                    \n",
    "                    layer.W=layer.W-(beta*m_W[i]+eta*layer.d_W[i])\n",
    "                    layer.b=layer.b-(beta*m_b[i]+eta*layer.d_b[i])\n",
    "                self.reset() #reset grads before new epoch\n",
    "                \n",
    "                \n",
    "    \n",
    "    def NAdam(self,X,Y,eta=1,beta1=0.9, beta2=0.999, batch_size=100,max_iters=1000):\n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    #updating momentum, velocity\n",
    "                    m_W[i]=beta1*m_W[i]+(1-beta1)*layer.d_W\n",
    "                    m_b[i]=beta1*m_b[i]+(1-beta1)*layer.d_b\n",
    "                    \n",
    "                    v_W[i]=beta2*v_W[i]+(1-beta2)*layer.d_W**2\n",
    "                    v_b[i]=beta2*v_b[i]+(1-beta2)*layer.d_b**2\n",
    "                    \n",
    "                    m_W_hat=m_W[i]/(1-np.power(beta1,t+1))\n",
    "                    m_b_hat=m_b[i]/(1-np.power(beta1,t+1))\n",
    "                    v_W_hat=v_W[i]/(1-np.power(beta2,t+1))\n",
    "                    v_b_hat=v_b[i]/(1-np.power(beta2,t+1))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    layer.W=layer.W-(eta/(np.sqrt(v_W_hat)+epsilon))*\\\n",
    "                    (beta1*m_W_hat+((1-beta1)/(1-np.power(beta1,t+1)))*layer.d_W)\n",
    "                    layer.b=layer.b-(eta/(np.sqrt(v_b_hat)+epsilon))*\\\n",
    "                    (beta1*m_b_hat+((1-beta1)/(1-np.power(beta1,t+1)))*layer.d_b)\n",
    "                    \n",
    "                self.reset() #reset grads before new epoch\n",
    "            \n",
    "\n",
    "                \n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd691a55-115a-45fb-8215-aa5667619b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_NN=optimizers(784,10,[32,32,32],['sigmoid','sigmoid','sigmoid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9eee0d7a-2dc2-449a-8e96-627bd2428f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test_NN.stochastic_gradient_descent(X,Y)\n",
    "# # test_NN.Momentum_based(X,Y)\n",
    "# # test_NN.batch_gradient_descent(X,Y,batch_size=3,max_iters=10000)\n",
    "# # test_NN.rmsprop(X,Y)\n",
    "# # test_NN.adam(X,Y,eta=0.001,batch_size=2,max_iters=10000)\n",
    "# test_NN.stochastic_gradient_descent(Xtrain,ytrain,eta=0.001,max_iters=10)\n",
    "# # test_NN.adam(Xtrain,ytrain,eta=0.001,batch_size=64,max_iters=10)\n",
    "# # test_NN.NAdam(Xtrain,ytrain,eta=0.001,batch_size=64,max_iters=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "833e5a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ypred0=test_NN.forward(Xtrain[:,[0]])\n",
    "# ypred1=test_NN.forward(Xtrain[:,[1]])\n",
    "# ypred2=test_NN.forward(Xtrain[:,[2]])\n",
    "# ypred=np.hstack((ypred0,ypred1,ypred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36c6020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y=np.hstack((ytrain[:,[0]],ytrain[:,[1]],ytrain[:,[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0416f0ef-fdfa-4848-be1f-32d72aebbe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep small laerning rate, batch size<shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90fef019",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.transpose([[1,2,-3,4],[4,-5,6,7],[2,3,4,-5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e26b0516",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.transpose([[0,0,1,0],[0,1,0,0],[0,0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60616e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [00:21<00:00, 4705.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE:- 4.00195210129486\n",
      "[[0.00097594 0.00097611 0.00097603]\n",
      " [0.33300268 0.33300267 0.33300268]\n",
      " [0.33275256 0.33275259 0.33275258]\n",
      " [0.33326881 0.33326864 0.33326872]] \n",
      " [[0 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.stochastic_gradient_descent(a,b,eta=0.001,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d166564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▏                                                          | 20479/100000 [00:04<00:16, 4940.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_NN\u001b[38;5;241m=\u001b[39moptimizers(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m4\u001b[39m,[\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m3\u001b[39m],[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtest_NN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m ypred0\u001b[38;5;241m=\u001b[39mtest_NN\u001b[38;5;241m.\u001b[39mforward(a[:,[\u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m      4\u001b[0m ypred1\u001b[38;5;241m=\u001b[39mtest_NN\u001b[38;5;241m.\u001b[39mforward(a[:,[\u001b[38;5;241m1\u001b[39m]])\n",
      "Cell \u001b[0;32mIn [23], line 22\u001b[0m, in \u001b[0;36moptimizers.batch_gradient_descent\u001b[0;34m(self, X, Y, eta, batch_size, max_iters)\u001b[0m\n\u001b[1;32m     20\u001b[0m y\u001b[38;5;241m=\u001b[39mY[:,[i]]\n\u001b[1;32m     21\u001b[0m y_pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m number_points_seen\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#update if the number of points seen==batch size, or if data ends\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [22], line 51\u001b[0m, in \u001b[0;36mModel.backward\u001b[0;34m(self, x, y, y_pred)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[idx\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39md_h\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmatmul(np\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[idx]\u001b[38;5;241m.\u001b[39mW),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[idx]\u001b[38;5;241m.\u001b[39md_a)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m#compute gradient -- element wise multiplivation, derivative of the activation function of layer idx-1\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[idx\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39md_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[idx\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39md_h\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_g\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39md_W\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39md_a,np\u001b[38;5;241m.\u001b[39mtranspose(x))\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39md_b\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39md_a\n",
      "Cell \u001b[0;32mIn [19], line 3\u001b[0m, in \u001b[0;36mget_activation_derivative.<locals>.sigmoid_d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid_d\u001b[39m(x):\n\u001b[0;32m----> 3\u001b[0m     sig\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(x \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mx)), \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(x)))\n\u001b[1;32m      4\u001b[0m     sig\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mnp\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mx))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sig\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39msig)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.batch_gradient_descent(a,b,eta=0.001,batch_size=2,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e757a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.Momentum_based(a,b,eta=0.001,batch_size=2,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e15f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.rmsprop(a,b,eta=0.001,batch_size=2,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b027f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.adam(a,b,eta=0.001,batch_size=2,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3197faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.NAG(a,b,eta=0.001,batch_size=2,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad19aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_NN=optimizers(4,4,[3,4,3],['sigmoid','sigmoid','sigmoid'])\n",
    "test_NN.NAdam(a,b,eta=0.001,batch_size=2,max_iters=100000)\n",
    "ypred0=test_NN.forward(a[:,[0]])\n",
    "ypred1=test_NN.forward(a[:,[1]])\n",
    "ypred2=test_NN.forward(a[:,[2]])\n",
    "ypred=np.hstack((ypred0,ypred1,ypred2))\n",
    "np.sum(np.abs(ypred-b))\n",
    "\n",
    "print('AE:-',np.sum(np.abs(ypred-b)))\n",
    "print(ypred,'\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87dca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
