{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34789f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2315322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d99502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.where(x >= 0, \n",
    "                        x, \n",
    "                        0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94c5e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(activation):#maybe getderivatives here iteself ?\n",
    "    def sigmoid(x):\n",
    "        return np.where(x >= 0, \n",
    "                        1 / (1 + np.exp(-x)), \n",
    "                        np.exp(x) / (1 + np.exp(x)))\n",
    "    def softmax(x):\n",
    "        z=x-np.max(x,axis=0)\n",
    "        return np.exp(z)/np.sum(np.exp(z),axis=0)\n",
    "    def relu(x):\n",
    "        rel=np.where(x >= 0, \n",
    "                            x, \n",
    "                            0)\n",
    "        return rel\n",
    "    if activation=='sigmoid':\n",
    "        return sigmoid\n",
    "    elif activation=='softmax':\n",
    "        return softmax\n",
    "    elif activation== 'tanh':\n",
    "        return np.tanh\n",
    "    elif activation== 'relu':\n",
    "        return relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f384261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_derivative(activation):#maybe getderivatives here iteself ?\n",
    "    def sigmoid_d(x):\n",
    "        sig= np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "        return sig*(1-sig)\n",
    "    def softmax_d(x):\n",
    "        z=x-np.max(x,axis=0)\n",
    "        soft=np.exp(z)/np.sum(np.exp(z),axis=0)\n",
    "        return soft*(1-soft)\n",
    "    def tanh_d(x):\n",
    "        return 1-np.tanh(x)**2\n",
    "    def relu_d(x):\n",
    "        return np.where(x >= 0, \n",
    "                            1, \n",
    "                            0)\n",
    "    \n",
    "    if activation=='sigmoid':\n",
    "        return sigmoid_d\n",
    "    elif activation=='softmax':\n",
    "        '''\n",
    "        need to think more, not required for backprop as we look directly at dL/da_l\n",
    "        But still, for the sake of completeness, and if user wants softmax in the middle?\n",
    "        d S(x_i) /d x_j= S(x_i)*(kronecker delta_i,j -S(x_j))\n",
    "        But we care about only dh_k,j/da_k,j So no need to implement d S(x_i) /d x_j\n",
    "        d S(x_i) /d x_i should suffice\n",
    "        so we get array of [ d S(x_1) /d x_1, d S(x_2) /d x_2, ....]\n",
    "        \n",
    "        For MSE loss after softmax, we need cross terms...\n",
    "        '''\n",
    "        \n",
    "        return softmax_d\n",
    "    elif activation=='tanh':\n",
    "        return tanh_d\n",
    "    elif activation=='relu':\n",
    "        return relu_d\n",
    "    assert(activation=='relu'or activation=='tanh'or activation=='sigmoid' or activation=='softmax'), 'Must be \\'relu\\'or \\'tanh\\' or \\'sigmoid\\' or \\'softmax\\' '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c0621d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(loss='cross-entropy'):\n",
    "    \n",
    "   \n",
    "    \n",
    "        \n",
    "    def crossentropy(P,Q):\n",
    "        assert(P.shape==Q.shape), \"Inputs must be of same shape\"\n",
    "\n",
    "        return np.sum([-np.dot(P[:,i],np.log2(Q[:,i])) for i in range(P.shape[1])])\n",
    "    def SE(P,Q):\n",
    "        assert(P.shape==Q.shape), \"Inputs must be of same shape\"\n",
    "\n",
    "        return np.sum(np.square(P-Q))\n",
    "    \n",
    "    if loss==\"SE\":\n",
    "        return SE\n",
    "    return crossentropy\n",
    "    \n",
    "    \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a254663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_derivative(loss):\n",
    "    def SE_d(y_in,y_pred_in):\n",
    "        '''\n",
    "        derivative of MSE after softmax is used to get probabs from a_L:\n",
    "        We need indicator because the all terms of y_true are required unlike cross-entropy where only y_pred[l] is required\n",
    "        Thus transforming the stacked indicator to y_true, not here...\n",
    "        \n",
    "        '''\n",
    "\n",
    "        def indicator(i,j):\n",
    "                if i==j:\n",
    "                    return 1\n",
    "                return 0\n",
    "\n",
    "\n",
    "        assert(y_in.shape[0]==y_pred_in.shape[0]),\"Inputs must contain same number of examples\"\n",
    "\n",
    "        y=y_in.ravel()\n",
    "        y_pred=y_pred_in.ravel()\n",
    "\n",
    "\n",
    "        return np.array([\n",
    "            [2*np.sum([(y_pred[i]-y[i])*y[i]*(indicator(i,j) - y_pred[j]) for i in range(y.shape[0])])]\n",
    "            for j in range(len(y))\n",
    "        ])    \n",
    "   \n",
    "    \n",
    "        \n",
    "    def crossentropy_d(y,y_pred):\n",
    "        \n",
    "\n",
    "        return -(y-y_pred)\n",
    "    \n",
    "    \n",
    "    if loss==\"cross-entropy\":\n",
    "        return crossentropy_d\n",
    "    return SE_d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "c48942c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self,input_size,output_size,activation='sigmoid',batch_size=2,type_='random'):\n",
    "            \n",
    "        ''' \n",
    "        output size number of neurons i\n",
    "        input size j\n",
    "        \n",
    "        '''\n",
    "        assert(type_=='random'or type_=='xavier'or type=='glorot' or type=='He' ), 'Must be \\'random\\'or \\'xavier\\' or \\'glorot\\' or \\'He\\' '\n",
    "        \n",
    "        if type_=='random':\n",
    "            scale=0.01\n",
    "            self.W=np.random.randn(output_size,input_size)*scale #size ixj\n",
    "            self.b=np.zeros((output_size,1))         #size i\n",
    "            \n",
    "        elif type_=='xavier' or type_=='glorot':\n",
    "            # Xavier Uniform\n",
    "            r=np.sqrt(6/(input_size+output_size))\n",
    "            self.W=np.random.uniform(-r,r,(output_size,input_size))\n",
    "            self.b=np.zeros((output_size,1))\n",
    "            \n",
    "        else:#He\n",
    "            self.W= np.random.randn(output_size,input_size)*np.sqrt(2/input_size)\n",
    "            self.b=np.zeros((output_size,1))\n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        self.a=np.zeros((output_size,batch_size))          #size i\n",
    "        self.h=np.zeros((output_size,batch_size))         #size i\n",
    "        self.g=get_activation(activation)\n",
    "        \n",
    "        self.d_a=np.zeros((output_size,batch_size))\n",
    "        self.d_h=np.zeros((output_size,batch_size))\n",
    "        self.d_W=np.zeros((output_size,input_size))\n",
    "        self.d_b=np.zeros((output_size,1))\n",
    "        self.d_g=get_activation_derivative(activation)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.a=self.b+np.matmul(self.W,inputs)\n",
    "        self.h=self.g(self.a)\n",
    "        return self.h\n",
    "    def reset(self):\n",
    "        self.d_a=np.zeros(np.shape(self.d_a))\n",
    "        self.d_h=np.zeros(np.shape(self.d_h))\n",
    "        self.d_W=np.zeros(np.shape(self.d_W))\n",
    "        self.d_b=np.zeros(np.shape(self.d_b))\n",
    "        \n",
    "    def hard_set(self,W,b):#hardsets the weight. useful for debugging\n",
    "        self.W=W\n",
    "        self.b=b\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "06f18d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,X_size,Y_size,hidden_layer_sizes=[4],hidden_layer_activations=['sigmoid'],loss='cross-entropy',lamdba_m=0,batch_size=4):\n",
    "        '''\n",
    "        '''   \n",
    "        \n",
    "        self.input_size=X_size\n",
    "        self.output_size=Y_size\n",
    "        self.hidden_layer_sizes=hidden_layer_sizes\n",
    "        self.layers=[]\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        prev_size=self.input_size\n",
    "        for size,activation in zip(hidden_layer_sizes,hidden_layer_activations):\n",
    "            self.layers.append(layer(prev_size,size,activation,batch_size))\n",
    "            prev_size=size\n",
    "        self.layers.append(layer(size,self.output_size,'softmax',batch_size))\n",
    "        \n",
    "        self.loss=get_loss(loss)#without regularization term\n",
    "        self.loss_d=get_loss_derivative(loss)\n",
    "        self.lamdba_m=lamdba_m #we shall pass lambda/m to this, where m is patch size\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output=x\n",
    "        \n",
    "        for layer in  self.layers:\n",
    "            output=layer.forward(output)  \n",
    "        return output\n",
    "    \n",
    "    def backward(self,x,y,y_pred):\n",
    "        # self.layers[-1].d_h is not needed as d_h is used to calculate d_a and self.layers[-1].h is softmax\n",
    "        self.layers[-1].d_a=self.loss_d(y,y_pred)\n",
    "            \n",
    "        \n",
    "        \n",
    "        for idx in range(len(self.layers)-1,0,-1): #goes from L->2, for l=1 we do outside\n",
    "            \n",
    "            \n",
    "            #compute gradient wrt parameters\n",
    "            self.layers[idx].d_W=np.dot(self.layers[idx].d_a,np.transpose(self.layers[idx-1].h))+self.lamdba_m*self.layers[idx].W\n",
    "            self.layers[idx].d_b=np.sum(self.layers[idx].d_a,axis=1,keepdims=True)\n",
    "            \n",
    "            #compute gradient wrt layer below -- will help in next layer iter\n",
    "            self.layers[idx-1].d_h=np.matmul(np.transpose(self.layers[idx].W),self.layers[idx].d_a)\n",
    "            \n",
    "            #compute gradient -- element wise multiplivation, derivative of the activation function of layer idx-1\n",
    "            self.layers[idx-1].d_a=self.layers[idx-1].d_h*self.layers[idx-1].d_g(self.layers[idx-1].a)\n",
    "        assert(idx-1==0)\n",
    "                        \n",
    "        self.layers[0].d_W=np.dot(self.layers[0].d_a,np.transpose(x))+self.lamdba_m*self.layers[0].W\n",
    "        self.layers[0].d_b=np.sum(self.layers[0].d_a,axis=1,keepdims=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self,Xtest):\n",
    "        \n",
    "        return self.forward(Xtest)\n",
    "    \n",
    "        \n",
    "        \n",
    "                    \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b7f728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "85416eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53984"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(0,54000-32,32)[-1]+32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "76db4320",
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizers_tester:\n",
    "    def __init__(self,X_size,Y_size,hidden_layer_sizes=[4],hidden_layer_activations=['sigmoid'],\n",
    "                 loss='cross-entropy',optimizer='adam',lamdba=0,batch_size=5,epochs=10,eta=1e-3):\n",
    "        \n",
    "#     def __init__(self,X_size,Y_size,hidden_layer_sizes=[4],hidden_layer_activations=['sigmoid'],\n",
    "#                  loss='cross-entropy',lamdba_m=0,batch_size=4)\n",
    "        self.batch_size=batch_size\n",
    "        self.epochs=epochs\n",
    "        self.train_loss=[]\n",
    "        self.val_loss=[]\n",
    "        self.model=Model(X_size,Y_size,hidden_layer_sizes,hidden_layer_activations,loss=loss,lamdba_m=lamdba/self.batch_size,batch_size=self.batch_size)\n",
    "        self.learning_rate=eta\n",
    "        self.optimizer=optimizer\n",
    "\n",
    "        \n",
    "\n",
    "    def batch_gradient_descent(self,traindat,testdat,verbose=False):\n",
    "        \n",
    "        '''\n",
    "        Mini-Batch Gradient Descent\n",
    "        at batchsize=1, behaves like sgd, batchsize=np.shape(X)[1], behaves as gd\n",
    "        eta is the learning rate\n",
    "        '''\n",
    "        X,Y=traindat\n",
    "        \n",
    "        Xval,Yval=testdat\n",
    "        WTrack=[]\n",
    "        bTrack=[]\n",
    "        dWTrack=[]\n",
    "        dbTrack=[]\n",
    "        \n",
    "        def update_batch():\n",
    "            for layer in self.model.layers:\n",
    "                layer.W=layer.W-self.learning_rate*layer.d_W\n",
    "                layer.b=layer.b-self.learning_rate*layer.d_b\n",
    "\n",
    "                WTrack_layer.append(layer.W)\n",
    "                bTrack_layer.append(layer.b)\n",
    "                dWTrack_layer.append(layer.d_W)\n",
    "                dbTrack_layer.append(layer.d_b)\n",
    "            dWTrack.append(dWTrack_layer)    \n",
    "            dbTrack.append(dbTrack_layer)\n",
    "            WTrack.append(WTrack_layer)    \n",
    "            bTrack.append(bTrack_layer)  \n",
    "        \n",
    "#         !!!!!!\n",
    "        \n",
    "\n",
    "        reminder=X.shape[1]%self.batch_size\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            for i in range(0,np.shape(X)[1]-self.batch_size,self.batch_size):\n",
    "                x=X[:,i:i+self.batch_size]\n",
    "                y=Y[:,i:i+self.batch_size]\n",
    "#                 print(x.shape)\n",
    "                y_pred=self.model.forward(x)\n",
    "    \n",
    "    \n",
    "                self.model.backward(x,y,y_pred)\n",
    "                dWTrack_layer=[]\n",
    "                dbTrack_layer=[]\n",
    "                WTrack_layer=[]\n",
    "                bTrack_layer=[]\n",
    "                update_batch()\n",
    "#                 for layer in self.model.layers:\n",
    "#                     layer.W=layer.W-self.learning_rate*layer.d_W\n",
    "#                     layer.b=layer.b-self.learning_rate*layer.d_b\n",
    "\n",
    "#                     WTrack_layer.append(layer.W)\n",
    "#                     bTrack_layer.append(layer.b)\n",
    "#                     dWTrack_layer.append(layer.d_W)\n",
    "#                     dbTrack_layer.append(layer.d_b)\n",
    "#                 dWTrack.append(dWTrack_layer)    \n",
    "#                 dbTrack.append(dbTrack_layer)\n",
    "#                 WTrack.append(WTrack_layer)    \n",
    "#                 bTrack.append(bTrack_layer) \n",
    "            if reminder:\n",
    "\n",
    "                x=np.hstack((X[:,i+self.batch_size:],X[:,:reminder]))\n",
    "                y=np.hstack((Y[:,i+self.batch_size:],Y[:,:reminder]))\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                dWTrack_layer=[]\n",
    "                dbTrack_layer=[]\n",
    "                WTrack_layer=[]\n",
    "                bTrack_layer=[]\n",
    "                update_batch()\n",
    "#                 for layer in self.model.layers:\n",
    "#                     layer.W=layer.W-self.learning_rate*layer.d_W\n",
    "#                     layer.b=layer.b-self.learning_rate*layer.d_b\n",
    "\n",
    "#                     WTrack_layer.append(layer.W)\n",
    "#                     bTrack_layer.append(layer.b)\n",
    "#                     dWTrack_layer.append(layer.d_W)\n",
    "#                     dbTrack_layer.append(layer.d_b)\n",
    "#                 dWTrack.append(dWTrack_layer)    \n",
    "#                 dbTrack.append(dbTrack_layer)\n",
    "#                 WTrack.append(WTrack_layer)    \n",
    "#                 bTrack.append(bTrack_layer) \n",
    "                \n",
    "\n",
    "                    \n",
    "                \n",
    "                #update if the number of points seen==batch size, or if data ends \n",
    "                    \n",
    "            \n",
    "            #end of epoch\n",
    "            print(Y.shape)\n",
    "            print(self.model.predict(X).shape)\n",
    "            regularization=1/2*self.model.lamdba_m*np.sum([np.sum(layer.W**2) for layer in self.model.layers])\n",
    "            self.train_loss.append((self.model.loss(Y,self.model.predict(X))+regularization)/X.shape[1])\n",
    "            self.val_loss.append(self.model.loss(Yval,self.model.predict(Xval))/Xval.shape[1])\n",
    "            \n",
    "        return dWTrack,dbTrack,WTrack,bTrack\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "2a7f1d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce=get_loss('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "65350a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_loss.<locals>.crossentropy(P, Q)>"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cda233",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "4d78258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "\n",
    "# np.random.seed(42)#sets a seed, used for reproducability\n",
    "\n",
    "def one_hot(inarray): #converts to one hot encoding\n",
    "    outarray = np.zeros((inarray.size, inarray.max() + 1))\n",
    "    outarray[np.arange(inarray.size), inarray] = 1\n",
    "    return outarray\n",
    "\n",
    "def Preprocess(X,y):\n",
    "      \n",
    "    '''Unrolls X,y, rehsapes into column vectors, one hots y'''\n",
    "    assert(X.shape[0]==y.shape[0]),\"Inputs must contain same number of examples, stored in rows\" #checks if same dim\n",
    "    \n",
    "    X_processed=np.reshape(X,(X.shape[0],784))/255\n",
    "    X_processed=X_processed.T\n",
    "    y_processed=one_hot(y).T\n",
    "    return np.array(X_processed),y_processed\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "Xtest,ytest=Preprocess(X_test,y_test)\n",
    "\n",
    "# def tran_val_split(X,y,split=0.1):\n",
    "#     assert(X.shape[1]==y.shape[1]), \"Inputs must contain same number of examples, stored in columns\"# as vectors are now stored in cols, do check if no of elemnts are equal\n",
    "#     len_split=int(np.shape(X)[1]*split)\n",
    "#     X_val=X[:,:len_split]\n",
    "#     y_val=y[:,:len_split]\n",
    "    \n",
    "#     X_train=X[:,len_split:]\n",
    "#     y_train=y[:,len_split:]\n",
    "    \n",
    "#     return (X_train,y_train),(X_val,y_val)\n",
    "    \n",
    "        \n",
    "\n",
    "# (Xtrain,ytrain),(Xval,yval)=tran_val_split(X_train_clean,y_train_clean)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain,Xval,ytrain,yval=train_test_split(X_train,y_train,test_size=0.1)\n",
    "Xtrain,ytrain=Preprocess(Xtrain,ytrain)\n",
    "Xval,yval=Preprocess(Xval,yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "ac6fa165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▋                                                                                       | 1/10 [00:00<00:05,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 54000)\n",
      "(10, 54000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████████████▍                                                                             | 2/10 [00:01<00:05,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 54000)\n",
      "(10, 54000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|█████████████████████████████                                                                    | 3/10 [00:01<00:04,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 54000)\n",
      "(10, 54000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████████████████████████████▊                                                          | 4/10 [00:02<00:03,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 54000)\n",
      "(10, 54000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████▌                                                | 5/10 [00:02<00:02,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 54000)\n",
      "(10, 54000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████████████▏                                      | 6/10 [00:03<00:02,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 54000)\n",
      "(10, 54000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████████████████████████████████████████████████████████████████▉                             | 7/10 [00:03<00:01,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 54000)\n",
      "(10, 54000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████████████████████████████████████████████▌                   | 8/10 [00:04<00:00,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 54000)\n",
      "(10, 54000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|███████████████████████████████████████████████████████████████████████████████████████▎         | 9/10 [00:04<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 54000)\n",
      "(10, 54000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:05<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 54000)\n",
      "(10, 54000)\n",
      "1.0295560050526589 1.0369981791947287\n",
      "Accuracy train 0.7648888888888888\n",
      "Accuracy val 0.7698333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt=optimizers_tester(Xtrain.shape[0],ytrain.shape[0],[32,32,32],['relu','relu','relu'],\n",
    "           loss='cross-entropy',optimizer='adam',\n",
    "           lamdba=0,batch_size=32,epochs=10,eta=1e-3)\n",
    "DW,DB,WT,BT=opt.batch_gradient_descent((Xtrain,ytrain),(Xval,yval))\n",
    "plt.plot(opt.train_loss)\n",
    "plt.plot(opt.val_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(opt.train_loss[-1],opt.val_loss[-1])\n",
    "\n",
    "print('Accuracy train',accuracy_check(opt.model.predict(Xtrain),ytrain))\n",
    "print('Accuracy val',accuracy_check(opt.model.predict(Xval),yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "0f8866f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mo=opt.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "32ac40b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00643727,  0.01173756, -0.01984132, -0.01004735,  0.00761189,\n",
       "        -0.01136236, -0.00603732,  0.01647874,  0.03484951, -0.00096917,\n",
       "         0.01035648,  0.00104815, -0.00296641,  0.00130846, -0.01028775,\n",
       "        -0.00344981, -0.00641499,  0.02827146,  0.0120024 ,  0.01618525,\n",
       "         0.00772579, -0.0068624 , -0.01977598, -0.0233357 , -0.00322569,\n",
       "         0.0032473 , -0.01130451, -0.00792189, -0.00216974,  0.00071263,\n",
       "        -0.01046685, -0.00543728],\n",
       "       [-0.00351945,  0.00607755,  0.01275921, -0.00736414,  0.02253749,\n",
       "        -0.01098852, -0.01117545, -0.00335224, -0.00173253, -0.0052884 ,\n",
       "         0.00142864, -0.01823779, -0.01441385,  0.01250853,  0.00563866,\n",
       "        -0.00251345,  0.01218685, -0.00739192, -0.00472472,  0.00136041,\n",
       "         0.02870576, -0.00822286,  0.00768264, -0.00596755, -0.00927878,\n",
       "         0.02176428,  0.00625695, -0.00633393, -0.00824067, -0.01498603,\n",
       "         0.00679035, -0.00831083],\n",
       "       [-0.00316369, -0.01166108,  0.00128403, -0.00198975,  0.00370252,\n",
       "         0.01404886,  0.00963413, -0.01032082,  0.0121734 ,  0.00601023,\n",
       "         0.00185379,  0.00241986,  0.01874103, -0.02269064,  0.01759596,\n",
       "        -0.00023106,  0.00243339, -0.02623026, -0.00676246, -0.01477847,\n",
       "         0.00656653,  0.00674904,  0.00980787,  0.00603191, -0.01696922,\n",
       "        -0.01107372, -0.00298466,  0.0204478 ,  0.00593706, -0.00571693,\n",
       "         0.01496366, -0.00646508],\n",
       "       [ 0.00236256,  0.01584895, -0.01281992,  0.01278115, -0.01583114,\n",
       "        -0.00953756, -0.00806604, -0.02230363,  0.00522698,  0.00466736,\n",
       "        -0.0206573 ,  0.01199824,  0.00473106, -0.00557678, -0.0151367 ,\n",
       "         0.000533  , -0.00974222,  0.02026575, -0.00633579,  0.01561869,\n",
       "        -0.01917578, -0.00257514,  0.00867328,  0.02420555,  0.00719363,\n",
       "        -0.00385941,  0.00582194,  0.00035427,  0.00888377,  0.01641219,\n",
       "        -0.00667656,  0.00504951],\n",
       "       [ 0.00129645, -0.0241606 ,  0.01283061,  0.00309595, -0.00388313,\n",
       "         0.00657462,  0.00387312,  0.00134182, -0.00473859, -0.01065999,\n",
       "        -0.00074156, -0.01240133, -0.01425513, -0.00523989,  0.01021995,\n",
       "        -0.00801809, -0.0023973 , -0.00743017,  0.00771391, -0.00100392,\n",
       "        -0.00011545,  0.01619004, -0.00860154, -0.00360065,  0.00838724,\n",
       "        -0.00554839,  0.01128382, -0.01279063, -0.0063393 ,  0.00915601,\n",
       "        -0.00691916, -0.01940712],\n",
       "       [ 0.00606571,  0.02069143, -0.01059572,  0.01113194, -0.00325085,\n",
       "        -0.02670078, -0.00066929, -0.00819417,  0.01390186,  0.02496105,\n",
       "         0.00819132, -0.0137568 , -0.00398685, -0.00200008, -0.01396634,\n",
       "        -0.01129481,  0.02418215,  0.01574956,  0.01411871, -0.02400315,\n",
       "        -0.00143017,  0.01899882,  0.00801589,  0.00321513, -0.00943604,\n",
       "         0.00423407, -0.00836375, -0.01588511,  0.005433  , -0.0067522 ,\n",
       "         0.00867318,  0.00781006],\n",
       "       [-0.01178829,  0.02180939, -0.00282807,  0.00428064, -0.01848004,\n",
       "         0.00529336,  0.00394138, -0.01063921, -0.03047195, -0.0096052 ,\n",
       "         0.00018584,  0.01463946,  0.01195695,  0.00116499,  0.00545856,\n",
       "         0.01189039, -0.00148882, -0.00314701,  0.00129244,  0.00775647,\n",
       "        -0.01166376, -0.0139609 , -0.00969824, -0.00655957, -0.00137157,\n",
       "         0.01088769,  0.00287507,  0.01124441,  0.0010287 ,  0.01029694,\n",
       "         0.00097301,  0.0030316 ],\n",
       "       [-0.0079981 , -0.00300151, -0.0004028 ,  0.01656177, -0.01163404,\n",
       "         0.00951811,  0.00401974,  0.01939852, -0.01644594, -0.02477319,\n",
       "        -0.00807685,  0.00373725,  0.00851132,  0.02181556, -0.00528377,\n",
       "        -0.00348771, -0.01131751, -0.0192641 , -0.00525491,  0.0019386 ,\n",
       "        -0.01153928,  0.00339459,  0.00121067,  0.00442488,  0.02119121,\n",
       "        -0.00717296, -0.00416345,  0.01235728,  0.00050754,  0.00666181,\n",
       "         0.01338237,  0.01231291],\n",
       "       [-0.01029857, -0.01652619,  0.0056557 , -0.02062274,  0.0053313 ,\n",
       "         0.0083765 ,  0.00847797,  0.01735036,  0.00253461, -0.00053736,\n",
       "         0.00214197,  0.01982363,  0.00011684, -0.00588089,  0.01952705,\n",
       "         0.01941292, -0.00350753, -0.0075981 , -0.00635107, -0.01007528,\n",
       "         0.01482166, -0.0031776 , -0.000771  , -0.0074272 ,  0.00672415,\n",
       "        -0.01391899,  0.00173151, -0.00514211,  0.00297902, -0.00813165,\n",
       "        -0.01709709,  0.01328511],\n",
       "       [ 0.02060612, -0.0208155 ,  0.0139583 , -0.00782745,  0.01389599,\n",
       "         0.01477777, -0.00399824,  0.00024062, -0.01529735,  0.01619466,\n",
       "         0.00531766, -0.00927067, -0.00843496,  0.00459074, -0.01376563,\n",
       "        -0.00284137, -0.00393401,  0.00677479, -0.00569852,  0.0070014 ,\n",
       "        -0.01389531, -0.0105336 ,  0.00345641,  0.00901321, -0.00321493,\n",
       "         0.00144014, -0.00115292,  0.00366991, -0.00801938, -0.00765277,\n",
       "        -0.00362292, -0.00186888]])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mo.layers[3].b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "1e3855ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(opt.model.layers[0].W,Xtrain[:,:32]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "47cb0bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_check(Y,Ypred):\n",
    "    return np.sum(np.argmax(Ypred,axis=0)==np.argmax(Y,axis=0))/Y.shape[1]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "00c9c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "da5612df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizers:\n",
    "    def __init__(self,X_size,Y_size,hidden_layer_sizes=[4],hidden_layer_activations=['sigmoid'],\n",
    "                 loss='cross-entropy',optimizer='adam',lamdba=0,batch_size=5,epochs=10,eta=1e-3):\n",
    "        self.batch_size=batch_size\n",
    "        self.epochs=epochs\n",
    "        self.train_loss=[]\n",
    "        self.val_loss=[]\n",
    "        self.model=Model(X_size,Y_size,hidden_layer_sizes,hidden_layer_activations,loss,lamdba/self.batch_size)\n",
    "        self.learning_rate=eta/batch_size\n",
    "        self.optimizer=optimizer\n",
    "\n",
    "        \n",
    "\n",
    "    def batch_gradient_descent(self,traindat,testdat,verbose=False):\n",
    "        \n",
    "        '''\n",
    "        Mini-Batch Gradient Descent\n",
    "        at batchsize=1, behaves like sgd, batchsize=np.shape(X), behaves as gd\n",
    "        eta is the learning rate\n",
    "        '''\n",
    "        X,Y=traindat\n",
    "        \n",
    "        Xval,Yval=testdat\n",
    "        \n",
    "        \n",
    "\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "                #update if the number of points seen==batch size, or if data ends\n",
    "                if number_points_seen%self.batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                    for layer in self.model.layers:\n",
    "                        layer.W=layer.W-self.learning_rate*layer.d_W\n",
    "                        layer.b=layer.b-self.learning_rate*layer.d_b\n",
    "                    self.model.reset() #reset grads before new epoch\n",
    "            \n",
    "            #end of epoch\n",
    "            regularization=1/2*self.model.lamdba_m*np.sum([np.sum(layer.W**2) for layer in self.model.layers])\n",
    "            self.train_loss.append((self.model.loss(Y,self.model.predict(X))+regularization)/X.shape[1])\n",
    "            self.val_loss.append(self.model.loss(Yval,self.model.predict(Xval))/Xval.shape[1])\n",
    "            \n",
    "    def stochastic_gradient_descent(self,traindat,testdat,eta=1e-3,max_iters=10):\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        stochastic Gradient Descent\n",
    "        '''\n",
    "                \n",
    "        X,Y=traindat\n",
    "        \n",
    "        Xval,Yval=testdat\n",
    "\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                \n",
    "                #update\n",
    "                for layer in self.model.layers:\n",
    "                    layer.W=layer.W-self.learning_rate*layer.d_W\n",
    "                    layer.b=layer.b-self.learning_rate*layer.d_b\n",
    "                self.model.reset() #reset grads before new update\n",
    "                    \n",
    "            regularization=1/2*self.model.lamdba_m*np.sum([np.sum(layer.W**2) for layer in self.model.layers])\n",
    "            self.train_loss.append((self.model.loss(Y,self.model.predict(X))+regularization)/X.shape[1])\n",
    "            self.val_loss.append(self.model.loss(Yval,self.model.predict(Xval))/Xval.shape[1])\n",
    "        \n",
    "    def Momentum(self,traindat,testdat,beta=0.9):\n",
    "        ''''''\n",
    "        \n",
    "        X,Y=traindat\n",
    "        \n",
    "        Xval,Yval=testdat\n",
    "        u_W=[np.zeros(np.shape(layer.d_W)) for layer in self.model.layers]\n",
    "        u_b=[np.zeros(np.shape(layer.d_b)) for layer in self.model.layers]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%self.batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.model.layers)):\n",
    "                    layer=self.model.layers[i]\n",
    "                    u_W[i]=beta*u_W[i]+layer.d_W\n",
    "                    u_b[i]=beta*u_b[i]+layer.d_b\n",
    "                    layer.W=layer.W-self.learning_rate*u_W[i]\n",
    "                    layer.b=layer.b-self.learning_rate*u_b[i]\n",
    "                self.model.reset() #reset grads before new update\n",
    "                \n",
    "            regularization=1/2*self.model.lamdba_m*np.sum([np.sum(layer.W**2) for layer in self.model.layers])\n",
    "            self.train_loss.append((self.model.loss(Y,self.model.predict(X))+regularization)/X.shape[1])\n",
    "            self.val_loss.append(self.model.loss(Yval,self.model.predict(Xval))/Xval.shape[1])\n",
    "                \n",
    "\n",
    "\n",
    "    def rmsprop(self,traindat,testdat,beta=0.9):\n",
    "        ''''''\n",
    "        X,Y=traindat\n",
    "        \n",
    "        Xval,Yval=testdat\n",
    "        \n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.model.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.model.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%self.batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.model.layers)):\n",
    "                    layer=self.model.layers[i]\n",
    "                    v_W[i]=beta*v_W[i]+(1-beta)*layer.d_W**2\n",
    "                    v_b[i]=beta*v_b[i]+(1-beta)*layer.d_b**2\n",
    "                    layer.W=layer.W-(self.learning_rate/np.sqrt(v_W[i]+epsilon))*layer.d_W\n",
    "                    layer.b=layer.b-(self.learning_rate/np.sqrt(v_b[i]+epsilon))*layer.d_b\n",
    "                self.model.reset() #reset grads before new epoch\n",
    "                \n",
    "            regularization=1/2*self.model.lamdba_m*np.sum([np.sum(layer.W**2) for layer in self.model.layers])\n",
    "            self.train_loss.append((self.model.loss(Y,self.model.predict(X))+regularization)/X.shape[1])\n",
    "            self.val_loss.append(self.model.loss(Yval,self.model.predict(Xval))/Xval.shape[1])\n",
    "\n",
    "            \n",
    "    def Adam(self,traindat,testdat,beta1=0.9, beta2=0.999):\n",
    "        ''''''\n",
    "        X,Y=traindat\n",
    "        \n",
    "        Xval,Yval=testdat\n",
    "        \n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.model.layers]\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.model.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.model.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.model.layers]\n",
    "        \n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%self.batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.model.layers)):\n",
    "                    layer=self.model.layers[i]\n",
    "                    #updating momentum, velocity\n",
    "                    m_W[i]=beta1*m_W[i]+(1-beta1)*layer.d_W\n",
    "                    m_b[i]=beta1*m_b[i]+(1-beta1)*layer.d_b\n",
    "                    \n",
    "                    v_W[i]=beta2*v_W[i]+(1-beta2)*layer.d_W**2\n",
    "                    v_b[i]=beta2*v_b[i]+(1-beta2)*layer.d_b**2\n",
    "                    \n",
    "                    m_W_hat=m_W[i]/(1-np.power(beta1,t+1))\n",
    "                    m_b_hat=m_b[i]/(1-np.power(beta1,t+1))\n",
    "                    v_W_hat=v_W[i]/(1-np.power(beta2,t+1))\n",
    "                    v_b_hat=v_b[i]/(1-np.power(beta2,t+1))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    layer.W=layer.W-(self.learning_rate*m_W_hat)/(np.sqrt(v_W_hat)+epsilon)\n",
    "                    layer.b=layer.b-(self.learning_rate*m_b_hat)/(np.sqrt(v_b_hat)+epsilon)\n",
    "                self.model.reset() #reset grads before new epoch\n",
    "                \n",
    "            regularization=1/2*self.model.lamdba_m*np.sum([np.sum(layer.W**2) for layer in self.model.layers])\n",
    "            self.train_loss.append((self.model.loss(Y,self.model.predict(X))+regularization)/X.shape[1])\n",
    "            self.val_loss.append(self.model.loss(Yval,self.model.predict(Xval))/Xval.shape[1])\n",
    "    \n",
    "    def NAG(self,traindat,testdat,beta=0.9):\n",
    "        \n",
    "        ''''''\n",
    "        \n",
    "        X,Y=traindat\n",
    "        \n",
    "        \n",
    "        Xval,Yval=testdat\n",
    "        \n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.model.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.model.layers]\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%self.batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.model.layers)):\n",
    "                    layer=self.model.layers[i]\n",
    "                    m_W[i]=beta*m_W[i]+self.learning_rate*layer.d_W\n",
    "                    m_b[i]=beta*m_b[i]+self.learning_rate*layer.d_b\n",
    "\n",
    "                    \n",
    "                    layer.W=layer.W-(beta*m_W[i]+self.learning_rate*layer.d_W[i])\n",
    "                    layer.b=layer.b-(beta*m_b[i]+self.learning_rate*layer.d_b[i])\n",
    "                self.model.reset() #reset grads before new epoch\n",
    "                \n",
    "            regularization=1/2*self.model.lamdba_m*np.sum([np.sum(layer.W**2) for layer in self.model.layers])\n",
    "            self.train_loss.append((self.model.loss(Y,self.model.predict(X))+regularization)/X.shape[1])\n",
    "            self.val_loss.append(self.model.loss(Yval,self.model.predict(Xval))/Xval.shape[1])\n",
    "    \n",
    "    def NAdam(self,traindat,testdat,beta1=0.9, beta2=0.999):\n",
    "        ''''''\n",
    "        \n",
    "        X,Y=traindat\n",
    "        \n",
    "        Xval,Yval=testdat\n",
    "        \n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.model.layers]\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.model.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.model.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.model.layers]\n",
    "        \n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%self.batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.model.layers)):\n",
    "                    layer=self.model.layers[i]\n",
    "                    #updating momentum, velocity\n",
    "                    m_W[i]=beta1*m_W[i]+(1-beta1)*layer.d_W\n",
    "                    m_b[i]=beta1*m_b[i]+(1-beta1)*layer.d_b\n",
    "                    \n",
    "                    v_W[i]=beta2*v_W[i]+(1-beta2)*layer.d_W**2\n",
    "                    v_b[i]=beta2*v_b[i]+(1-beta2)*layer.d_b**2\n",
    "                    \n",
    "                    m_W_hat=m_W[i]/(1-np.power(beta1,t+1))\n",
    "                    m_b_hat=m_b[i]/(1-np.power(beta1,t+1))\n",
    "                    v_W_hat=v_W[i]/(1-np.power(beta2,t+1))\n",
    "                    v_b_hat=v_b[i]/(1-np.power(beta2,t+1))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    layer.W=layer.W-(self.learning_rate/(np.sqrt(v_W_hat)+epsilon))*\\\n",
    "                    (beta1*m_W_hat+((1-beta1)/(1-np.power(beta1,t+1)))*layer.d_W)\n",
    "                    layer.b=layer.b-(self.learning_rate/(np.sqrt(v_b_hat)+epsilon))*\\\n",
    "                    (beta1*m_b_hat+((1-beta1)/(1-np.power(beta1,t+1)))*layer.d_b)\n",
    "                    \n",
    "                self.model.reset() #reset grads before new epoch\n",
    "            regularization=1/2*self.model.lamdba_m*np.sum([np.sum(layer.W**2) for layer in self.model.layers])\n",
    "            self.train_loss.append((self.model.loss(Y,self.model.predict(X))+regularization)/X.shape[1])\n",
    "            self.val_loss.append(self.model.loss(Yval,self.model.predict(Xval))/Xval.shape[1])\n",
    "\n",
    "                \n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2fdede",
   "metadata": {},
   "source": [
    "# Shapap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "39249513",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "dd85de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mo=Model(784,10,[5,5,5],['relu','relu','relu'],batch_size=16,loss='cross-entropy',lamdba_m=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cad2e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ypred=Mo.forward(Xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7106da37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(Ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a2e2dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Mo.backward(Xx,Yy,Mo.forward(Xx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "83e393f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -1.82539178e-08,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mo.layers[0].d_W/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "28c66d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "gce=get_loss_derivative('cross-entropy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
