{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34789f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2315322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94c5e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(activation):#maybe getderivatives here iteself ?\n",
    "    def sigmoid(x):\n",
    "        return np.where(x >= 0, \n",
    "                        1 / (1 + np.exp(-x)), \n",
    "                        np.exp(x) / (1 + np.exp(x)))\n",
    "    def softmax(x):\n",
    "        z=x-np.max(x,axis=0)\n",
    "        return np.exp(z)/np.sum(np.exp(z),axis=0)\n",
    "    if activation=='sigmoid':\n",
    "        return sigmoid\n",
    "    elif activation=='softmax':\n",
    "        return softmax\n",
    "    elif activation== 'tanh':\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f384261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_derivative(activation):#maybe getderivatives here iteself ?\n",
    "    def sigmoid_d(x):\n",
    "        sig= np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "        return sig*(1-sig)\n",
    "    def softmax_d(x):\n",
    "        z=x-np.max(x,axis=0)\n",
    "        soft=np.exp(z)/np.sum(np.exp(z),axis=0)\n",
    "        return soft*(1-soft)\n",
    "    if activation=='sigmoid':\n",
    "        return sigmoid_d\n",
    "    elif activation=='softmax':\n",
    "        '''\n",
    "        need to think more, not required for backprop as we look directly at dL/da_l\n",
    "        But still, for the sake of completeness, and if user wants softmax in the middle?\n",
    "        d S(x_i) /d x_j= S(x_i)*(kronecker delta_i,j -S(x_j))\n",
    "        But we care about only dh_k,j/da_k,j So no need to implement d S(x_i) /d x_j\n",
    "        d S(x_i) /d x_i should suffice\n",
    "        so we get array of [ d S(x_1) /d x_1, d S(x_2) /d x_2, ....]\n",
    "        \n",
    "        For MSE loss after softmax, we need cross terms...\n",
    "        '''\n",
    "        \n",
    "        return softmax_d\n",
    "    elif activation=='tanh':\n",
    "        return 1-tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c0621d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(loss='cross-entropy'):\n",
    "    \n",
    "   \n",
    "    \n",
    "        \n",
    "    def crossentropy(P,Q):\n",
    "        assert(P.shape==Q.shape), \"Inputs must be of same shape\"\n",
    "\n",
    "        return np.sum([-np.dot(P[:,i],np.log2(Q[:,i])) for i in range(P.shape[1])])\n",
    "    def SE(P,Q):\n",
    "        assert(P.shape==Q.shape), \"Inputs must be of same shape\"\n",
    "\n",
    "        return np.sum(np.square(P-Q))\n",
    "    \n",
    "    if loss==\"SE\":\n",
    "        return SE\n",
    "    return crossentropy\n",
    "    \n",
    "    \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a254663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_derivative(loss):\n",
    "    def SE_d(y_in,y_pred_in):\n",
    "        '''\n",
    "        derivative of MSE after softmax is used to get probabs from a_L:\n",
    "        We need indicator because the all terms of y_true are required unlike cross-entropy where only y_pred[l] is required\n",
    "        Thus transforming the stacked indicator to y_true, not here...\n",
    "        \n",
    "        '''\n",
    "\n",
    "        def indicator(i,j):\n",
    "                if i==j:\n",
    "                    return 1\n",
    "                return 0\n",
    "\n",
    "\n",
    "        assert(y_in.shape[0]==y_pred_in.shape[0]),\"Inputs must contain same number of examples\"\n",
    "\n",
    "        y=y_in.ravel()\n",
    "        y_pred=y_pred_in.ravel()\n",
    "\n",
    "\n",
    "        return np.array([\n",
    "            [2*np.sum([(y_pred[i]-y[i])*y[i]*(indicator(i,j) - y_pred[j]) for i in range(y.shape[0])])]\n",
    "            for j in range(len(y))\n",
    "        ])    \n",
    "   \n",
    "    \n",
    "        \n",
    "    def crossentropy_d(y,y_pred):\n",
    "        \n",
    "\n",
    "        return -(y-y_pred)\n",
    "    \n",
    "    \n",
    "    if loss==\"cross-entropy\":\n",
    "        return crossentropy_d\n",
    "    return SE_d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c48942c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self,input_size,output_size,activation='sigmoid'):\n",
    "            \n",
    "        ''' \n",
    "        output size number of neurons i\n",
    "        input size j\n",
    "        \n",
    "        '''\n",
    "        self.W=np.random.randn(output_size,input_size) #size ixj\n",
    "        self.b=np.random.randn(output_size,1)           #size i\n",
    "        self.a=np.random.randn(output_size,1)           #size i\n",
    "        self.h=np.random.randn(output_size,1)           #size i\n",
    "        self.g=get_activation(activation)\n",
    "        \n",
    "        self.d_a=np.zeros((output_size,1))\n",
    "        self.d_h=np.zeros((output_size,1))\n",
    "        self.d_W=np.zeros((output_size,input_size))\n",
    "        self.d_b=np.zeros((output_size,1))\n",
    "        self.d_g=get_activation_derivative(activation)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.a=self.b+np.matmul(self.W,inputs)\n",
    "        self.h=self.g(self.a)\n",
    "        return self.h\n",
    "    def reset(self):\n",
    "        self.d_a=np.zeros(np.shape(self.d_a))\n",
    "        self.d_h=np.zeros(np.shape(self.d_h))\n",
    "        self.d_W=np.zeros(np.shape(self.d_W))\n",
    "        self.d_b=np.zeros(np.shape(self.d_b))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06f18d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,X_size,Y_size,hidden_layer_sizes=[4],hidden_layer_activations=['sigmoid'],loss='cross-entropy',lamdba_m=0):\n",
    "        '''\n",
    "        '''   \n",
    "        \n",
    "        self.input_size=X_size\n",
    "        self.output_size=Y_size\n",
    "        self.hidden_layer_sizes=hidden_layer_sizes\n",
    "        self.layers=[]\n",
    "        \n",
    "        prev_size=self.input_size\n",
    "    \n",
    "        for size,activation in zip(hidden_layer_sizes,hidden_layer_activations):\n",
    "            self.layers.append(layer(prev_size,size,activation))\n",
    "            prev_size=size\n",
    "        self.layers.append(layer(size,self.output_size,'softmax'))\n",
    "        \n",
    "        self.loss=get_loss(loss)#without regularization term\n",
    "        self.loss_d=get_loss_derivative(loss)\n",
    "        self.lamdba_m=lamdba_m #we shall pass lambda/m to this, where m is patch size\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output=x\n",
    "        # print(output.shape)\n",
    "        for layer in  self.layers:\n",
    "            # print('W',layer.W.shape)\n",
    "            output=layer.forward(output)\n",
    "            # print(output.shape)   \n",
    "        return output\n",
    "    \n",
    "    def reset(self):\n",
    "        for layer in  self.layers:\n",
    "            # resets the dWs\n",
    "            layer.reset()\n",
    "    \n",
    " \n",
    "        \n",
    "    \n",
    "          \n",
    "            \n",
    "    def backward(self,x,y,y_pred):\n",
    "        # self.layers[-1].d_h is not needed as d_h is used to calculate d_a and self.layers[-1].h is softmax\n",
    "        self.layers[-1].d_a=self.loss_d(y,y_pred)\n",
    "            \n",
    "        \n",
    "        \n",
    "        for idx in range(len(self.layers)-1,0,-1): #goes from L->2, for l=1 we do outside\n",
    "            \n",
    "            \n",
    "            #compute gradient wrt parameters\n",
    "            self.layers[idx].d_W+=np.matmul(self.layers[idx].d_a,np.transpose(self.layers[idx-1].h))+self.lamdba_m*self.layers[idx].W\n",
    "            self.layers[idx].d_b+=self.layers[idx].d_a\n",
    "            \n",
    "            #compute gradient wrt layer below -- will help in next layer iter\n",
    "            self.layers[idx-1].d_h=np.matmul(np.transpose(self.layers[idx].W),self.layers[idx].d_a)\n",
    "            \n",
    "            #compute gradient -- element wise multiplivation, derivative of the activation function of layer idx-1\n",
    "            self.layers[idx-1].d_a=self.layers[idx-1].d_h*self.layers[idx-1].d_g(self.layers[idx-1].a)\n",
    "                        \n",
    "        self.layers[0].d_W+=np.matmul(self.layers[0].d_a,np.transpose(x))+self.lamdba_m*self.layers[0].W\n",
    "        self.layers[0].d_b+=self.layers[0].d_a\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self,Xtest):\n",
    "        preds=[]\n",
    "        for i in range(Xtest.shape[1]):\n",
    "            preds.append(self.forward(Xtest[:,[i]]))\n",
    "        \n",
    "        ytest_pred=np.hstack(preds)\n",
    "        return ytest_pred\n",
    "    \n",
    "        \n",
    "        \n",
    "                    \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b41087c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizers(Model):\n",
    "    def __init__(self,X_size,Y_size,hidden_layer_sizes=[4],hidden_layer_activations=['sigmoid']):\n",
    "        super().__init__(X_size,Y_size,hidden_layer_sizes,hidden_layer_activations)\n",
    "\n",
    "    def batch_gradient_descent(self,X,Y,eta=1,batch_size=1,max_iters=1000):\n",
    "        \n",
    "        '''\n",
    "        Mini-Batch Gradient Descent\n",
    "        at batchsize=1, behaves like sgd, batchsize=np.shape(X), behaves as gd\n",
    "        eta is the learning rate\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "                #update if the number of points seen==batch size, or if data ends\n",
    "                if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                    for layer in self.layers:\n",
    "                        layer.W=layer.W-eta*layer.d_W\n",
    "                        layer.b=layer.b-eta*layer.d_b\n",
    "                    self.reset() #reset grads before new epoch\n",
    "    \n",
    "            \n",
    "    def stochastic_gradient_descent(self,X,Y,eta=1,max_iters=1000):\n",
    "        \n",
    "        '''\n",
    "        stochastic Gradient Descent\n",
    "        '''\n",
    "        t=1        \n",
    "        \n",
    "\n",
    "        while t<max_iters:\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                \n",
    "                #update\n",
    "                for layer in self.layers:\n",
    "                    layer.W=layer.W-eta*layer.d_W\n",
    "                    layer.b=layer.b-eta*layer.d_b\n",
    "            t+=1\n",
    "        \n",
    "        \n",
    "    def Momentum_based(self,X,Y,eta=1,beta=0.9,batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        u_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        u_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        for t in range(max_iters):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    u_W[i]=beta*u_W[i]+layer.d_W\n",
    "                    u_b[i]=beta*u_b[i]+layer.d_b\n",
    "                    layer.W=layer.W-eta*u_W[i]\n",
    "                    layer.b=layer.b-eta*u_b[i]\n",
    "                self.reset() #reset grads before new epoch\n",
    "                \n",
    "\n",
    "\n",
    "    def rmsprop(self,X,Y,eta=1,beta=0.9,batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in range(max_iters):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    v_W[i]=beta*v_W[i]+(1-beta)*layer.d_W**2\n",
    "                    v_b[i]=beta*v_b[i]+(1-beta)*layer.d_b**2\n",
    "                    layer.W=layer.W-(eta/np.sqrt(v_W[i]+epsilon))*layer.d_W\n",
    "                    layer.b=layer.b-(eta/np.sqrt(v_b[i]+epsilon))*layer.d_b\n",
    "                self.reset() #reset grads before new epoch\n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "    def adam(self,X,Y,eta=1,beta1=0.9, beta2=0.999, batch_size=100,max_iters=1000):\n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in range(max_iters):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    #updating momentum, velocity\n",
    "                    m_W[i]=beta1*m_W[i]+(1-beta1)*layer.d_W\n",
    "                    m_b[i]=beta1*m_b[i]+(1-beta1)*layer.d_b\n",
    "                    \n",
    "                    v_W[i]=beta2*v_W[i]+(1-beta2)*layer.d_W**2\n",
    "                    v_b[i]=beta2*v_b[i]+(1-beta2)*layer.d_b**2\n",
    "                    \n",
    "                    m_W_hat=m_W[i]/(1-np.power(beta1,t+1))\n",
    "                    m_b_hat=m_b[i]/(1-np.power(beta1,t+1))\n",
    "                    v_W_hat=v_W[i]/(1-np.power(beta2,t+1))\n",
    "                    v_b_hat=v_b[i]/(1-np.power(beta2,t+1))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    layer.W=layer.W-(eta*m_W_hat)/(np.sqrt(v_W_hat)+epsilon)\n",
    "                    layer.b=layer.b-(eta*m_b_hat)/(np.sqrt(v_b_hat)+epsilon)\n",
    "                self.reset() #reset grads before new epoch\n",
    "    \n",
    "    def NAG(self,X,Y,eta=1,beta=0.9,batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        for t in range(max_iters):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    m_W[i]=beta*m_W[i]+eta*layer.d_W\n",
    "                    m_b[i]=beta*m_b[i]+eta*layer.d_b\n",
    "\n",
    "                    \n",
    "                    layer.W=layer.W-(beta*m_W[i]+eta*layer.d_W[i])\n",
    "                    layer.b=layer.b-(beta*m_b[i]+eta*layer.d_b[i])\n",
    "                self.reset() #reset grads before new epoch\n",
    "                \n",
    "                \n",
    "    def NAdam(self,X,Y,eta=1,beta=0.9,batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        for t in range(max_iters):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    m_W[i]=beta*m_W[i]+eta*layer.d_W\n",
    "                    m_b[i]=beta*m_b[i]+eta*layer.d_b\n",
    "\n",
    "                    \n",
    "                    layer.W=layer.W-(beta*m_W[i]+eta*layer.d_W[i])\n",
    "                    layer.b=layer.b-(beta*m_b[i]+eta*layer.d_b[i])\n",
    "                self.reset() #reset grads before new epoch\n",
    "                \n",
    "\n",
    "                \n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48418053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizers_beta:\n",
    "    def __init__(self,X_size,Y_size,hidden_layer_sizes=[4],hidden_layer_activations=['sigmoid'],\n",
    "                 loss='cross-entropy',optimizer='adam',lamdba=0,batch_size=5,epochs=10,eta=1e-3):\n",
    "        self.batch_size=batch_size\n",
    "        self.epochs=epochs\n",
    "        self.train_loss=[]\n",
    "        self.val_loss=[]\n",
    "        self.model=Model(X_size,Y_size,hidden_layer_sizes,hidden_layer_activations,loss,lamdba/self.batch_size)\n",
    "        self.learning_rate=eta\n",
    "        self.optimizer=optimizer\n",
    "        \n",
    "\n",
    "    def batch_gradient_descent(self,traindat,testdat,verbose=False):\n",
    "        \n",
    "        '''\n",
    "        Mini-Batch Gradient Descent\n",
    "        at batchsize=1, behaves like sgd, batchsize=np.shape(X), behaves as gd\n",
    "        eta is the learning rate\n",
    "        '''\n",
    "        X,Y=traindat\n",
    "        \n",
    "        Xval,Yval=testdat\n",
    "        eta=self.learning_rate\n",
    "        \n",
    "\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "                #update if the number of points seen==batch size, or if data ends\n",
    "                if number_points_seen%self.batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                    for layer in self.model.layers:\n",
    "                        layer.W=layer.W-eta*layer.d_W\n",
    "                        layer.b=layer.b-eta*layer.d_b\n",
    "                    self.model.reset() #reset grads before new epoch\n",
    "            \n",
    "            #end of epoch\n",
    "            regularization=1/2*self.model.lamdba_m*np.sum([np.sum(layer.W**2) for layer in self.model.layers])\n",
    "            self.train_loss.append(self.model.loss(Y,self.model.predict(X)))\n",
    "            self.val_loss.append(self.model.loss(Yval,self.model.predict(Xval)))\n",
    "            \n",
    "    def stochastic_gradient_descent(self,X,Y,eta=1e-3,max_iters=10):\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        stochastic Gradient Descent\n",
    "        '''\n",
    "                \n",
    "        \n",
    "\n",
    "        for t in tqdm(range(self.epochs)):\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.model.forward(x)\n",
    "                self.model.backward(x,y,y_pred)\n",
    "                \n",
    "                #update\n",
    "                for layer in self.model.layers:\n",
    "                    layer.W=layer.W-self.eta*layer.d_W\n",
    "                    layer.b=layer.b-self.eta*layer.d_b\n",
    "                self.model.reset() #reset grads before new update\n",
    "                    \n",
    "            \n",
    "        \n",
    "        \n",
    "    def Momentum(self,X,Y,eta=1e-3,beta=0.9,batch_size=100,max_iters=10):\n",
    "        ''''''\n",
    "        u_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        u_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    u_W[i]=beta*u_W[i]+layer.d_W\n",
    "                    u_b[i]=beta*u_b[i]+layer.d_b\n",
    "                    layer.W=layer.W-eta*u_W[i]\n",
    "                    layer.b=layer.b-eta*u_b[i]\n",
    "                self.reset() #reset grads before new update\n",
    "                \n",
    "\n",
    "\n",
    "    def rmsprop(self,X,Y,eta=1e-3,beta=0.9,batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    v_W[i]=beta*v_W[i]+(1-beta)*layer.d_W**2\n",
    "                    v_b[i]=beta*v_b[i]+(1-beta)*layer.d_b**2\n",
    "                    layer.W=layer.W-(eta/np.sqrt(v_W[i]+epsilon))*layer.d_W\n",
    "                    layer.b=layer.b-(eta/np.sqrt(v_b[i]+epsilon))*layer.d_b\n",
    "                self.reset() #reset grads before new epoch\n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "    def Adam(self,X,Y,eta=1e-3,beta1=0.9, beta2=0.999, batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    #updating momentum, velocity\n",
    "                    m_W[i]=beta1*m_W[i]+(1-beta1)*layer.d_W\n",
    "                    m_b[i]=beta1*m_b[i]+(1-beta1)*layer.d_b\n",
    "                    \n",
    "                    v_W[i]=beta2*v_W[i]+(1-beta2)*layer.d_W**2\n",
    "                    v_b[i]=beta2*v_b[i]+(1-beta2)*layer.d_b**2\n",
    "                    \n",
    "                    m_W_hat=m_W[i]/(1-np.power(beta1,t+1))\n",
    "                    m_b_hat=m_b[i]/(1-np.power(beta1,t+1))\n",
    "                    v_W_hat=v_W[i]/(1-np.power(beta2,t+1))\n",
    "                    v_b_hat=v_b[i]/(1-np.power(beta2,t+1))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    layer.W=layer.W-(eta*m_W_hat)/(np.sqrt(v_W_hat)+epsilon)\n",
    "                    layer.b=layer.b-(eta*m_b_hat)/(np.sqrt(v_b_hat)+epsilon)\n",
    "                self.reset() #reset grads before new epoch\n",
    "    \n",
    "    def NAG(self,X,Y,eta=1e-3,beta=0.9,batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    m_W[i]=beta*m_W[i]+eta*layer.d_W\n",
    "                    m_b[i]=beta*m_b[i]+eta*layer.d_b\n",
    "\n",
    "                    \n",
    "                    layer.W=layer.W-(beta*m_W[i]+eta*layer.d_W[i])\n",
    "                    layer.b=layer.b-(beta*m_b[i]+eta*layer.d_b[i])\n",
    "                self.reset() #reset grads before new epoch\n",
    "                \n",
    "                \n",
    "    \n",
    "    def NAdam(self,X,Y,eta=1e-3,beta1=0.9, beta2=0.999, batch_size=100,max_iters=1000):\n",
    "        ''''''\n",
    "        m_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        v_W=[np.zeros(np.shape(layer.d_W)) for layer in self.layers]\n",
    "        m_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        v_b=[np.zeros(np.shape(layer.d_b)) for layer in self.layers]\n",
    "        t=0\n",
    "        epsilon=1e-10\n",
    "\n",
    "\n",
    "\n",
    "        for t in tqdm(range(max_iters)):\n",
    "            number_points_seen=0\n",
    "            for i in range(np.shape(X)[1]):\n",
    "                x=X[:,[i]]\n",
    "                y=Y[:,[i]]\n",
    "                y_pred=self.forward(x)\n",
    "                self.backward(x,y,y_pred)\n",
    "                number_points_seen+=1\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            #update if the number of points seen==batch size, or if data ends\n",
    "            if number_points_seen%batch_size==0 or number_points_seen==np.shape(X)[1]:\n",
    "                for i in range(len(self.layers)):\n",
    "                    layer=self.layers[i]\n",
    "                    #updating momentum, velocity\n",
    "                    m_W[i]=beta1*m_W[i]+(1-beta1)*layer.d_W\n",
    "                    m_b[i]=beta1*m_b[i]+(1-beta1)*layer.d_b\n",
    "                    \n",
    "                    v_W[i]=beta2*v_W[i]+(1-beta2)*layer.d_W**2\n",
    "                    v_b[i]=beta2*v_b[i]+(1-beta2)*layer.d_b**2\n",
    "                    \n",
    "                    m_W_hat=m_W[i]/(1-np.power(beta1,t+1))\n",
    "                    m_b_hat=m_b[i]/(1-np.power(beta1,t+1))\n",
    "                    v_W_hat=v_W[i]/(1-np.power(beta2,t+1))\n",
    "                    v_b_hat=v_b[i]/(1-np.power(beta2,t+1))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    layer.W=layer.W-(eta/(np.sqrt(v_W_hat)+epsilon))*\\\n",
    "                    (beta1*m_W_hat+((1-beta1)/(1-np.power(beta1,t+1)))*layer.d_W)\n",
    "                    layer.b=layer.b-(eta/(np.sqrt(v_b_hat)+epsilon))*\\\n",
    "                    (beta1*m_b_hat+((1-beta1)/(1-np.power(beta1,t+1)))*layer.d_b)\n",
    "                    \n",
    "                self.reset() #reset grads before new epoch\n",
    "            \n",
    "\n",
    "                \n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a7f1d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce=get_loss('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65350a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_loss.<locals>.crossentropy(P, Q)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cda233",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d78258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 19:48:49.920796: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 19:48:50.082916: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-14 19:48:50.143066: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-14 19:48:50.613666: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-14 19:48:50.613752: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-14 19:48:50.613755: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "y_train.shape\n",
    "\n",
    "# np.random.seed(42)#sets a seed, used for reproducability\n",
    "\n",
    "def one_hot(inarray): #converts to one hot encoding\n",
    "    outarray = np.zeros((inarray.size, inarray.max() + 1))\n",
    "    outarray[np.arange(inarray.size), inarray] = 1\n",
    "    return outarray\n",
    "\n",
    "def Preprocess(X,y):\n",
    "      \n",
    "    '''Unrolls X,y, rehsapes into column vectors, one hots y'''\n",
    "    assert(X.shape[0]==y.shape[0]),\"Inputs must contain same number of examples, stored in rows\" #checks if same dim\n",
    "    X_processed=[]\n",
    "    y_processed=[]\n",
    "    \n",
    "    for i in range(np.shape(X)[0]):\n",
    "        X_processed.append(X_train[i].ravel())\n",
    "    y_processed=one_hot(y_train).T\n",
    "    return np.array(X_processed).T,y_processed\n",
    "        \n",
    "    \n",
    "\n",
    "X_train_clean,y_train_clean=Preprocess(X_train,y_train)\n",
    "\n",
    "Xtest,ytest=Preprocess(X_test,y_test)\n",
    "\n",
    "def tran_val_split(X,y,split=0.1):\n",
    "    assert(X.shape[1]==y.shape[1]), \"Inputs must contain same number of examples, stored in columns\"# as vectors are now stored in cols, do check if no of elemnts are equal\n",
    "    len_split=int(np.shape(X)[1]*split)\n",
    "    np.random.shuffle(X)\n",
    "    np.random.shuffle(y)\n",
    "    X_val=X[:,:len_split]\n",
    "    y_val=y[:,:len_split]\n",
    "    \n",
    "    X_train=X[:,len_split:]\n",
    "    y_train=y[:,len_split:]\n",
    "    \n",
    "    return (X_train,y_train),(X_val,y_val)\n",
    "    \n",
    "        \n",
    "\n",
    "(Xtrain,ytrain),(Xval,yval)=tran_val_split(X_train_clean,y_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e8d5d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 54000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "707b1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt=optimizers_beta(Xtrain.shape[0],ytrain.shape[0],[32,32,32],['sigmoid','sigmoid','sigmoid'],\n",
    "#            loss='cross-entropy',optimizer='adam',\n",
    "#            lamdba=0,batch_size=1,epochs=10,eta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58e0878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=optimizers_beta(Xtrain.shape[0],ytrain.shape[0],[32,32,32],['sigmoid','sigmoid','sigmoid'],\n",
    "           loss='cross-entropy',optimizer='adam',\n",
    "           lamdba=0,batch_size=32,epochs=10,eta=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3aa815ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/10 [00:00<?, ?it/s]/tmp/ipykernel_77022/3051356264.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  1 / (1 + np.exp(-x)),\n",
      "/tmp/ipykernel_77022/3051356264.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(x) / (1 + np.exp(x)))\n",
      "/tmp/ipykernel_77022/3051356264.py:5: RuntimeWarning: invalid value encountered in divide\n",
      "  np.exp(x) / (1 + np.exp(x)))\n",
      "/tmp/ipykernel_77022/2350236407.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  sig= np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
      "/tmp/ipykernel_77022/2350236407.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  sig= np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 10/10 [01:23<00:00,  8.31s/it]\n"
     ]
    }
   ],
   "source": [
    "opt.batch_gradient_descent((Xtrain,ytrain),(Xval,yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08fe4f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[322805.2359087452,\n",
       " 531313.5646198248,\n",
       " 445667.56811491423,\n",
       " 222694.88637149567,\n",
       " 183595.06620853578,\n",
       " 279122.328975694,\n",
       " 249508.04431981017,\n",
       " 254534.962245438,\n",
       " 249392.34856481053,\n",
       " 263148.97526966874]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b066dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2884544.2969182497,\n",
       " 4769274.520104318,\n",
       " 4045681.837492031,\n",
       " 2029661.2594245973,\n",
       " 1684965.241022373,\n",
       " 2474201.804578602,\n",
       " 2248473.362420984,\n",
       " 2287151.4451650083,\n",
       " 2173963.1414595135,\n",
       " 2410422.736082721]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "22de5afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 54000)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f7031b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_77022/3051356264.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  1 / (1 + np.exp(-x)),\n",
      "/tmp/ipykernel_77022/3051356264.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(x) / (1 + np.exp(x)))\n",
      "/tmp/ipykernel_77022/3051356264.py:5: RuntimeWarning: invalid value encountered in divide\n",
      "  np.exp(x) / (1 + np.exp(x)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.94677972e-26],\n",
       "       [3.25007365e-09],\n",
       "       [1.51760453e-22],\n",
       "       [5.73102655e-08],\n",
       "       [7.20332878e-16],\n",
       "       [8.17388217e-01],\n",
       "       [1.82610995e-01],\n",
       "       [7.27308614e-07],\n",
       "       [2.64370996e-26],\n",
       "       [6.40370059e-24]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.model.predict(Xtrain[:,[3\n",
    "                           ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d84457c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain[:,[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bb3203cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/10 [00:00<?, ?it/s]/tmp/ipykernel_707411/3051356264.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  1 / (1 + np.exp(-x)),\n",
      "/tmp/ipykernel_707411/3051356264.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(x) / (1 + np.exp(x)))\n",
      "/tmp/ipykernel_707411/3051356264.py:5: RuntimeWarning: invalid value encountered in divide\n",
      "  np.exp(x) / (1 + np.exp(x)))\n",
      "/tmp/ipykernel_707411/2350236407.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  sig= np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
      "/tmp/ipykernel_707411/2350236407.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  sig= np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [01:01<00:00,  6.19s/it]\n"
     ]
    }
   ],
   "source": [
    "optl=optimizers(Xtrain.shape[0],ytrain.shape[0],[32,32,32],['sigmoid','sigmoid','sigmoid'],)\n",
    "optl.batch_gradient_descent(Xtrain,ytrain,max_iters=10,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "16a9d641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain[:,[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "66180708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_707411/3051356264.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  1 / (1 + np.exp(-x)),\n",
      "/tmp/ipykernel_707411/3051356264.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(x) / (1 + np.exp(x)))\n",
      "/tmp/ipykernel_707411/3051356264.py:5: RuntimeWarning: invalid value encountered in divide\n",
      "  np.exp(x) / (1 + np.exp(x)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.00367103e-18],\n",
       "       [3.54753173e-12],\n",
       "       [1.02516943e-27],\n",
       "       [1.57570798e-12],\n",
       "       [4.54712588e-16],\n",
       "       [9.99994626e-01],\n",
       "       [6.52285718e-10],\n",
       "       [3.88804446e-33],\n",
       "       [5.37285486e-06],\n",
       "       [8.80865142e-17]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optl.predict(Xtrain[:,[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
